{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data_utils.transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.data_utils.preprocessing import *\n",
    "from relax.utils import get_config, gumbel_softmax\n",
    "from relax.import_essentials import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import sklearn.preprocessing as skp\n",
    "from fastcore.test import test_fail\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BaseTransformation:\n",
    "    \"\"\"Base class for all transformations.\"\"\"\n",
    "    \n",
    "    def __init__(self, name, transformer: DataPreprocessor = None):\n",
    "        self.name = name\n",
    "        self.transformer = transformer\n",
    "\n",
    "    @property\n",
    "    def is_categorical(self) -> bool:   raise NotImplementedError\n",
    "\n",
    "    def fit(self, xs, y=None):          raise NotImplementedError\n",
    "\n",
    "    def transform(self, xs):            raise NotImplementedError\n",
    "\n",
    "    def fit_transform(self, xs, y=None):raise NotImplementedError\n",
    "\n",
    "    def inverse_transform(self, xs):    raise NotImplementedError\n",
    "\n",
    "    def apply_constraints(self, xs, cfs, hard, rng_key, **kwargs):\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def compute_reg_loss(self, xs, cfs, hard: bool = False):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def from_dict(self, params):        raise NotImplementedError    \n",
    "\n",
    "    def to_dict(self):                  raise NotImplementedError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _DefaultTransformation(BaseTransformation):\n",
    "\n",
    "    @property\n",
    "    def is_categorical(self) -> bool:\n",
    "        if self.transformer is None:\n",
    "            return False\n",
    "        return isinstance(self.transformer, EncoderPreprocessor)\n",
    "\n",
    "    def fit(self, xs, y=None):\n",
    "        if self.transformer is not None:\n",
    "            self.transformer.fit(xs)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        if self.transformer is None:\n",
    "            return xs\n",
    "        return self.transformer.transform(xs)\n",
    "\n",
    "    def fit_transform(self, xs, y=None):\n",
    "        if self.transformer is None:\n",
    "            return xs\n",
    "        return self.transformer.fit_transform(xs)\n",
    "    \n",
    "    def inverse_transform(self, xs):\n",
    "        if self.transformer is None:\n",
    "            return xs\n",
    "        return self.transformer.inverse_transform(xs)\n",
    "\n",
    "    def apply_constraints(self, xs: jax.Array, cfs: jax.Array, hard: bool = False, \n",
    "                          rng_key: jrand.PRNGKey = None, **kwargs):\n",
    "        return cfs\n",
    "    \n",
    "    def compute_reg_loss(self, xs, cfs, hard: bool = False):\n",
    "        return 0.\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        self.name = params[\"name\"]\n",
    "        if not 'transformer' in params.keys():\n",
    "            self.transformer = None\n",
    "        else:\n",
    "            self.transformer.from_dict(params[\"transformer\"])\n",
    "        return self\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\"name\": self.name, \"transformer\": self.transformer.to_dict()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MinMaxTransformation(_DefaultTransformation):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"minmax\", MinMaxScaler())\n",
    "\n",
    "    def apply_constraints(self, xs, cfs, **kwargs):\n",
    "        return np.clip(cfs, 0., 1.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.randn(100, 1)\n",
    "minmax_t = MinMaxTransformation()\n",
    "transformed_xs = minmax_t.fit_transform(xs)\n",
    "assert np.allclose(minmax_t.inverse_transform(transformed_xs), xs)\n",
    "assert minmax_t.is_categorical is False\n",
    "\n",
    "x = np.random.randn(100, 1)\n",
    "cf_constrained = minmax_t.apply_constraints(xs, x)\n",
    "assert np.all(cf_constrained >= 0) and np.all(cf_constrained <= 1)\n",
    "\n",
    "# Test from_dict and to_dict\n",
    "scaler_1 = MinMaxTransformation().from_dict(minmax_t.to_dict())\n",
    "assert np.allclose(minmax_t.transform(xs), scaler_1.transform(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class _OneHotTransformation(_DefaultTransformation):\n",
    "    def __init__(self, name: str = None):\n",
    "        super().__init__(name, OneHotEncoder())\n",
    "\n",
    "    @property\n",
    "    def num_categories(self) -> int:\n",
    "        return len(self.transformer.categories_)\n",
    "    \n",
    "    def hard_constraints(self, operand: tuple[jax.Array, jrand.PRNGKey, dict]): \n",
    "        x, rng_key, kwargs = operand\n",
    "        return jax.nn.one_hot(jnp.argmax(x, axis=-1), self.num_categories)\n",
    "    \n",
    "    def soft_constraints(self, operand: tuple[jax.Array, jrand.PRNGKey, dict]):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def apply_constraints(self, xs, cfs, hard: bool = False, rng_key=None, **kwargs):\n",
    "        return jax.lax.cond(\n",
    "            hard,\n",
    "            true_fun=self.hard_constraints,\n",
    "            false_fun=self.soft_constraints,\n",
    "            operand=(cfs, rng_key, kwargs),\n",
    "        )\n",
    "    \n",
    "    def compute_reg_loss(self, xs, cfs, hard: bool = False):\n",
    "        reg_loss_per_xs = (cfs.sum(axis=-1, keepdims=True) - 1.0) ** 2\n",
    "        return reg_loss_per_xs.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SoftmaxTransformation(_OneHotTransformation):\n",
    "    def __init__(self): \n",
    "        super().__init__(\"ohe\")\n",
    "\n",
    "    def soft_constraints(self, operand: tuple[jax.Array, jrand.PRNGKey, dict]):\n",
    "        x, rng_key, kwargs = operand\n",
    "        return jax.nn.softmax(x, axis=-1)\n",
    "    \n",
    "class GumbelSoftmaxTransformation(_OneHotTransformation):\n",
    "    \"\"\"Apply Gumbel softmax tricks for categorical transformation.\"\"\"\n",
    "\n",
    "    def __init__(self, tau: float = .1):\n",
    "        super().__init__(\"gumbel\")\n",
    "        self.tau = tau\n",
    "    \n",
    "    def soft_constraints(self, operand: tuple[jax.Array, jrand.PRNGKey, dict]):\n",
    "        x, rng_key, _ = operand\n",
    "        if rng_key is None: # No randomness\n",
    "            rng_key = jrand.PRNGKey(get_config().global_seed)\n",
    "        return gumbel_softmax(rng_key, x, self.tau)\n",
    "    \n",
    "    def apply_constraints(self, xs, cfs, hard: bool = False, rng_key=None, **kwargs):\n",
    "        \"\"\"Apply constraints to the counterfactuals. If `rng_key` is None, no randomness is used.\"\"\"\n",
    "        return super().apply_constraints(xs, cfs, hard, rng_key, **kwargs)\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return super().to_dict() | {\"tau\": self.tau}\n",
    "    \n",
    "def OneHotTransformation():\n",
    "    warnings.warn(\"OneHotTransformation is deprecated since v0.2.5. \"\n",
    "                  \"Use `SoftmaxTransformation`.\", DeprecationWarning)\n",
    "    return SoftmaxTransformation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_ohe_t(ohe_cls):\n",
    "    xs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\n",
    "    ohe_t = ohe_cls().fit(xs)\n",
    "    transformed_xs = ohe_t.transform(xs)\n",
    "    rng_key = jax.random.PRNGKey(get_config().global_seed)\n",
    "    assert ohe_t.is_categorical\n",
    "\n",
    "    x = jax.random.uniform(rng_key, shape=(100, 3))\n",
    "    # Test hard=True which applies softmax function.\n",
    "    soft = ohe_t.apply_constraints(transformed_xs, x, hard=False, rng_key=rng_key)\n",
    "    assert jnp.allclose(soft.sum(axis=-1), 1)\n",
    "    assert jnp.all(soft >= 0)\n",
    "    assert jnp.all(soft <= 1)\n",
    "    assert jnp.allclose(jnp.zeros((len(x), 1)), ohe_t.compute_reg_loss(xs, soft, hard=False))\n",
    "    assert jnp.allclose(soft, ohe_t.apply_constraints(transformed_xs, x, hard=False))\n",
    "\n",
    "    # Test hard=True which enforce one-hot constraint.\n",
    "    hard = ohe_t.apply_constraints(transformed_xs, x, hard=True, rng_key=rng_key)\n",
    "    assert np.all([1 in x for x in hard])\n",
    "    assert np.all([0 in x for x in hard])\n",
    "    assert jnp.allclose(hard.sum(axis=-1), 1)\n",
    "    assert jnp.allclose(jnp.zeros((len(x), 1)), ohe_t.compute_reg_loss(xs, hard, hard=False))\n",
    "\n",
    "    # Test compute_reg_loss\n",
    "    assert jnp.ndim(ohe_t.compute_reg_loss(xs, soft, hard=False)) == 0\n",
    "\n",
    "    # Test from_dict and to_dict\n",
    "    ohe_t_1 = ohe_cls().from_dict(ohe_t.to_dict())\n",
    "    assert np.allclose(ohe_t.transform(xs), ohe_t_1.transform(xs))\n",
    "\n",
    "\n",
    "test_ohe_t(SoftmaxTransformation)\n",
    "test_ohe_t(GumbelSoftmaxTransformation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OrdinalTransformation(_DefaultTransformation):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"ordinal\", OrdinalPreprocessor())\n",
    "\n",
    "    @property\n",
    "    def num_categories(self) -> int:\n",
    "        return len(self.transformer.categories_)\n",
    "    \n",
    "class IdentityTransformation(_DefaultTransformation):\n",
    "    def __init__(self):\n",
    "        super().__init__(\"identity\", None)\n",
    "\n",
    "    def fit(self, xs, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        return xs\n",
    "    \n",
    "    def fit_transform(self, xs, y=None):\n",
    "        return xs\n",
    "\n",
    "    def apply_constraints(self, xs, cfs, **kwargs):\n",
    "        return cfs\n",
    "    \n",
    "    def to_dict(self):\n",
    "        return {'name': 'identity'}\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        self.name = params[\"name\"]\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\n",
    "encoder = OrdinalTransformation().fit(xs)\n",
    "transformed_xs = encoder.transform(xs)\n",
    "assert np.all(encoder.inverse_transform(transformed_xs) == xs)\n",
    "assert encoder.is_categorical\n",
    "\n",
    "# Test from_dict and to_dict\n",
    "encoder_1 = OrdinalTransformation().from_dict(encoder.to_dict())\n",
    "assert np.allclose(encoder.transform(xs), encoder_1.transform(xs))\n",
    "\n",
    "xs = np.random.randn(100, 1)\n",
    "scaler = IdentityTransformation()\n",
    "transformed_xs = scaler.fit_transform(xs)\n",
    "assert np.all(transformed_xs == xs)\n",
    "\n",
    "# Test from_dict and to_dict\n",
    "scaler_1 = IdentityTransformation().from_dict(scaler.to_dict())\n",
    "assert np.allclose(scaler.transform(xs), scaler_1.transform(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "FEATURE_TRANSFORMATIONS = {\n",
    "    'ohe': SoftmaxTransformation,\n",
    "    'softmax': SoftmaxTransformation,\n",
    "    'gumbel': GumbelSoftmaxTransformation,\n",
    "    'minmax': MinMaxTransformation,\n",
    "    'ordinal': OrdinalTransformation,\n",
    "    'identity': IdentityTransformation,\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
