{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessors\n",
    "\n",
    "`DataPreprocessor` transforms *individual* features into numerical representations for the machine learning and recourse generation workflows. \n",
    "It can be considered as a drop-in jax-friendly replacement to the \n",
    "[sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module.\n",
    "The supported preprocessing methods include `MinMaxScaler` and `OneHotEncoder`. \n",
    "\n",
    ":::{.callout-important}\n",
    "\n",
    "Unlike the `DataPreprocessor` [sklearn.preprocessing](https://scikit-learn.org/stable/modules/classes.html#module-sklearn.preprocessing) module,\n",
    "all of the data preprocessors work only with single features (e.g., Dim: `(B, 1)`). \n",
    "\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp data_utils.preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#| hide\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "import numpy as np\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import sklearn.preprocessing as skp\n",
    "from fastcore.test import test_fail\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def _check_xs(xs: np.ndarray, name: str):\n",
    "    if xs.ndim > 2 or (xs.ndim == 2 and xs.shape[1] != 1):\n",
    "        raise ValueError(f\"`{name}` only supports array with a single feature, but got shape={xs.shape}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class DataPreprocessor:\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        name: str = None # The name of the preprocessor. If None, the class name will be used.\n",
    "    ):\n",
    "        \"\"\"Base class for data preprocessors.\"\"\"\n",
    "        self.name = name or self.__class__.__name__\n",
    "    \n",
    "    def fit(self, xs, y=None):\n",
    "        \"\"\"Fit the preprocessor with `xs` and `y`.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        \"\"\"Transform `xs`.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def fit_transform(self, xs, y=None):\n",
    "        \"\"\"Fit the preprocessor with `xs` and `y`, then transform `xs`.\"\"\"\n",
    "        self.fit(xs, y)\n",
    "        return self.transform(xs)\n",
    "    \n",
    "    def inverse_transform(self, xs):\n",
    "        \"\"\"Inverse transform `xs`.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        \"\"\"Convert the preprocessor to a dictionary.\"\"\"\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        \"\"\"Load the attributes of the preprocessor from a dictionary.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    __ALL__ = [\"fit\", \"transform\", \"fit_transform\", \"inverse_transform\", \"to_dict\", \"from_dict\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class MinMaxScaler(DataPreprocessor): \n",
    "    def __init__(self):\n",
    "        super().__init__(name=\"minmax\")\n",
    "        \n",
    "    def fit(self, xs, y=None):\n",
    "        _check_xs(xs, name=\"MinMaxScaler\")\n",
    "        self.min_ = xs.min(axis=0)\n",
    "        self.max_ = xs.max(axis=0)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        return (xs - self.min_) / (self.max_ - self.min_)\n",
    "    \n",
    "    def inverse_transform(self, xs):\n",
    "        return xs * (self.max_ - self.min_) + self.min_\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        self.min_ = params[\"min_\"]\n",
    "        self.max_ = params[\"max_\"]\n",
    "        return self\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\"min_\": self.min_, \"max_\": self.max_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.randn(100, )\n",
    "scaler = MinMaxScaler()\n",
    "transformed_xs = scaler.fit_transform(xs)\n",
    "assert transformed_xs.shape == (100, )\n",
    "assert np.allclose(xs, scaler.inverse_transform(transformed_xs))\n",
    "# Test correctness \n",
    "assert np.allclose(\n",
    "    transformed_xs, \n",
    "    skp.MinMaxScaler().fit_transform(xs.reshape(100, 1)).reshape(100,)\n",
    ")\n",
    "# Also work with 2D array\n",
    "xs = xs.reshape(100, 1)\n",
    "scaler = MinMaxScaler()\n",
    "transformed_xs = scaler.fit_transform(xs)\n",
    "assert np.allclose(xs, scaler.inverse_transform(transformed_xs))\n",
    "assert np.allclose(\n",
    "    transformed_xs, \n",
    "    skp.MinMaxScaler().fit_transform(xs.reshape(100, 1))\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`MinMaxScaler` only supports scaling a single feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = xs.reshape(50, 2)\n",
    "scaler = MinMaxScaler()\n",
    "test_fail(lambda: scaler.fit_transform(xs), \n",
    "          contains=\"`MinMaxScaler` only supports array with a single feature\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert to a dictionary (or the pytree representations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = xs.reshape(-1, 1)\n",
    "scaler = MinMaxScaler().fit(xs)\n",
    "scaler_1 = MinMaxScaler().from_dict(scaler.to_dict())\n",
    "assert np.allclose(scaler.transform(xs), scaler_1.transform(xs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "def _unique(xs):\n",
    "    if xs.dtype == object:\n",
    "        # Note: np.unique does not work with object dtype\n",
    "        # We will enforce xs to be string type\n",
    "        # It assumes that xs is a list of strings, and might not work\n",
    "        # for other cases (e.g., list of string and numbers)\n",
    "        return np.unique(xs.astype(str))\n",
    "    return np.unique(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class EncoderPreprocessor(DataPreprocessor):\n",
    "    \"\"\"Encode categorical features as an integer array.\"\"\"\n",
    "    def _fit(self, xs, y=None):\n",
    "        _check_xs(xs, name=\"EncoderPreprocessor\")\n",
    "        self.categories_ = _unique(xs)\n",
    "\n",
    "    def _transform(self, xs):\n",
    "        \"\"\"Transform data to ordinal encoding.\"\"\"\n",
    "        if xs.dtype == object:\n",
    "            xs = xs.astype(str)\n",
    "        ordinal = np.searchsorted(self.categories_, xs)\n",
    "        # return einops.rearrange(ordinal, 'k n -> n k')\n",
    "        return ordinal\n",
    "    \n",
    "    def _inverse_transform(self, xs):\n",
    "        \"\"\"Transform ordinal encoded data back to original data.\"\"\"\n",
    "        return self.categories_[xs.T].T\n",
    "    \n",
    "    def from_dict(self, params: dict):\n",
    "        self.categories_ = params[\"categories_\"]\n",
    "        return self\n",
    "    \n",
    "    def to_dict(self) -> dict:\n",
    "        return {\"categories_\": self.categories_}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OrdinalPreprocessor(EncoderPreprocessor):\n",
    "    \"\"\"Ordinal encoder for a single feature.\"\"\"\n",
    "    \n",
    "    def fit(self, xs, y=None):\n",
    "        self._fit(xs, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, xs):\n",
    "        if xs.ndim == 1:\n",
    "            raise ValueError(f\"OrdinalPreprocessor only supports 2D array with a single feature, \"\n",
    "                             f\"but got shape={xs.shape}.\")\n",
    "        return self._transform(xs)\n",
    "    \n",
    "    def inverse_transform(self, xs):\n",
    "        return self._inverse_transform(xs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\n",
    "enc = OrdinalPreprocessor().fit(xs)\n",
    "transformed_xs = enc.transform(xs)\n",
    "assert np.all(enc.inverse_transform(transformed_xs) == xs)\n",
    "# Test from_dict and to_dict\n",
    "enc_1 = OrdinalPreprocessor().from_dict(enc.to_dict())\n",
    "assert np.all(enc.transform(xs) == enc_1.transform(xs))\n",
    "\n",
    "xs = np.array(['a', 'b', 'c', np.nan, 'a', 'b', 'c', np.nan], dtype=object).reshape(-1, 1)\n",
    "enc = OrdinalPreprocessor().fit(xs)\n",
    "# Check categories_\n",
    "assert np.array_equiv(enc.categories_, np.array(['a', 'b', 'c', np.nan], dtype=str)) \n",
    "transformed_xs = enc.transform(xs)\n",
    "assert transformed_xs.shape == (8, 1)\n",
    "inverse_transformed_xs = enc.inverse_transform(transformed_xs)\n",
    "assert np.all(inverse_transformed_xs == xs.astype(str))\n",
    "# Test from_dict and to_dict\n",
    "enc_1 = OrdinalPreprocessor().from_dict(enc.to_dict())\n",
    "assert np.all(enc.transform(xs) == enc_1.transform(xs))\n",
    "assert np.array_equal(enc.categories_, enc_1.categories_)\n",
    "\n",
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, ))\n",
    "test_fail(lambda: OrdinalPreprocessor().fit_transform(xs), \n",
    "    contains=\"OrdinalPreprocessor only supports 2D array with a single feature\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class OneHotEncoder(EncoderPreprocessor):\n",
    "    \"\"\"One-hot encoder for a single categorical feature.\"\"\"\n",
    "    \n",
    "    def fit(self, xs, y=None):\n",
    "        self._fit(xs, y)\n",
    "        return self\n",
    "\n",
    "    def transform(self, xs):\n",
    "        if xs.ndim == 1:\n",
    "            raise ValueError(f\"OneHotEncoder only supports 2D array with a single feature, \"\n",
    "                             f\"but got shape={xs.shape}.\")\n",
    "        xs_int = self._transform(xs)\n",
    "        one_hot_feats = np.eye(len(self.categories_))[xs_int]\n",
    "        return einops.rearrange(one_hot_feats, 'n k d -> n (k d)')\n",
    "\n",
    "    def inverse_transform(self, xs):\n",
    "        xs_int = np.argmax(xs, axis=-1)\n",
    "        return self._inverse_transform(xs_int).reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, 1))\n",
    "enc = OneHotEncoder().fit(xs)\n",
    "transformed_xs = enc.transform(xs)\n",
    "assert np.all(enc.inverse_transform(transformed_xs) == xs)\n",
    "# Test from_dict and to_dict\n",
    "enc_1 = OneHotEncoder().from_dict(enc.to_dict())\n",
    "assert np.all(enc.transform(xs) == enc_1.transform(xs))\n",
    "\n",
    "xs = np.array(['a', 'b', 'c', np.nan, 'a', 'b', 'c', np.nan], dtype=object).reshape(-1, 1)\n",
    "enc = OneHotEncoder().fit(xs)\n",
    "# Check categories_\n",
    "assert np.array_equiv(enc.categories_, np.array(['a', 'b', 'c', np.nan], dtype=str)) \n",
    "transformed_xs = enc.transform(xs)\n",
    "assert np.all(enc.inverse_transform(transformed_xs) == xs.astype(str))\n",
    "assert np.array_equal(\n",
    "    transformed_xs, skp.OneHotEncoder(sparse_output=False).fit_transform(xs)\n",
    ") \n",
    "# Test from_dict and to_dict\n",
    "enc_1 = OneHotEncoder().from_dict(enc.to_dict())\n",
    "enc_2 = OneHotEncoder()\n",
    "enc_2.from_dict(enc_1.to_dict())\n",
    "assert np.all(enc.transform(xs) == enc_1.transform(xs))\n",
    "assert np.all(enc.transform(xs) == enc_2.transform(xs))\n",
    "\n",
    "xs = np.random.choice(['a', 'b', 'c'], size=(100, ))\n",
    "test_fail(lambda: OneHotEncoder().fit_transform(xs), \n",
    "    contains=\"OneHotEncoder only supports 2D array with a single feature\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
