{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vanilla CF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | default_exp methods.vanilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    }
   ],
   "source": [
    "# | export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import CFModule\n",
    "from relax.base import BaseConfig\n",
    "from relax.utils import auto_reshaping, grad_update, validate_configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from sklearn.datasets import make_classification\n",
    "from sklearn.model_selection import train_test_split\n",
    "from relax.ml_model import MLModule\n",
    "from relax.data_module import load_data\n",
    "from relax.ml_model import load_ml_module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# | exporti\n",
    "@auto_reshaping('x')\n",
    "def _vanilla_cf(\n",
    "    x: jnp.DeviceArray,  # `x` shape: (k,), where `k` is the number of features\n",
    "    y_target: Array, # `y_target` shape: (1,)\n",
    "    pred_fn: Callable[[Array], Array],  # y = pred_fn(x)\n",
    "    n_steps: int,\n",
    "    lr: float,  # learning rate for each `cf` optimization step\n",
    "    lambda_: float,  #  loss = validity_loss + lambda_params * cost\n",
    "    validity_fn: Callable,\n",
    "    cost_fn: Callable,\n",
    "    apply_constraints_fn: Callable\n",
    ") -> jnp.DeviceArray:  # return `cf` shape: (k,)\n",
    "    @jit\n",
    "    def loss_fn_1(y_true: Array, y_pred: Array):\n",
    "        return validity_fn(y_true, y_pred).mean()\n",
    "\n",
    "    @jit\n",
    "    def loss_fn_2(x: Array, cf: Array):\n",
    "        return cost_fn(cf, x).mean()\n",
    "\n",
    "    @partial(jit, static_argnums=(2,))\n",
    "    def loss_fn(\n",
    "        cf: Array,  # `cf` shape: (k, 1)\n",
    "        x: Array,  # `x` shape: (k, 1)\n",
    "        pred_fn: Callable[[Array], Array],\n",
    "    ):\n",
    "        y_pred = pred_fn(x)\n",
    "        cf_y_true = 1.0 - y_pred\n",
    "        cf_y_pred = pred_fn(cf)\n",
    "        return loss_fn_1(cf_y_true, cf_y_pred) + lambda_ * loss_fn_2(x, cf)\n",
    "\n",
    "    @loop_tqdm(n_steps)\n",
    "    def gen_cf_step(\n",
    "        i, cf_opt_state: Tuple[Array, optax.OptState] #x: Array, cf: Array, opt_state: optax.OptState\n",
    "    ) -> Tuple[jnp.DeviceArray, optax.OptState]:\n",
    "        cf, opt_state = cf_opt_state\n",
    "        cf_grads = jax.grad(loss_fn)(cf, x, pred_fn)\n",
    "        cf, opt_state = grad_update(cf_grads, cf, opt_state, opt)\n",
    "        cf = apply_constraints_fn(x, cf, hard=False)\n",
    "        return cf, opt_state\n",
    "\n",
    "    cf = jnp.array(x, copy=True)\n",
    "    opt = optax.rmsprop(lr)\n",
    "    opt_state = opt.init(cf)\n",
    "    # for _ in tqdm(range(n_steps)):\n",
    "    #     cf, opt_state = gen_cf_step(x, cf, opt_state)\n",
    "    cf, opt_state = lax.fori_loop(0, n_steps, gen_cf_step, (cf, opt_state))\n",
    "\n",
    "    cf = apply_constraints_fn(x, cf, hard=True)\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VanillaCFConfig(BaseConfig):\n",
    "    n_steps: int = 100\n",
    "    lr: float = 0.1\n",
    "    lambda_: float = 0.1\n",
    "    validity_fn: str = 'KLDivergence'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class VanillaCF(CFModule):\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        configs: dict | VanillaCFConfig = None,\n",
    "        name: str = None,\n",
    "    ):\n",
    "        if configs is None:\n",
    "            configs = VanillaCFConfig()\n",
    "        configs = validate_configs(configs, VanillaCFConfig)\n",
    "        name = \"VanillaCF\" if name is None else name\n",
    "        super().__init__(configs, name=name)\n",
    "\n",
    "    @auto_reshaping('x')\n",
    "    def generate_cf(\n",
    "        self,\n",
    "        x: Array,  # `x` shape: (k,), where `k` is the number of features\n",
    "        pred_fn: Callable[[Array], Array],\n",
    "        y_target: Array = None,\n",
    "    ) -> jnp.DeviceArray:\n",
    "        # TODO: Currently assumes binary classification.\n",
    "        if y_target is None:\n",
    "            y_target = 1 - pred_fn(x)\n",
    "        else:\n",
    "            y_target = jnp.array(y_target, copy=True)\n",
    "\n",
    "        return _vanilla_cf(\n",
    "            x=x,  # `x` shape: (k,), where `k` is the number of features\n",
    "            y_target=y_target,  # `y_target` shape: (1,)\n",
    "            pred_fn=pred_fn,  # y = pred_fn(x)\n",
    "            n_steps=self.config.n_steps,\n",
    "            lr=self.config.lr,  # learning rate for each `cf` optimization step\n",
    "            lambda_=self.config.lambda_,  #  loss = validity_loss + lambda_params * cost\n",
    "            validity_fn=keras.losses.get({'class_name': self.config.validity_fn, 'config': {'reduction': None}}),\n",
    "            cost_fn=keras.losses.get({'class_name': 'MeanSquaredError', 'config': {'reduction': None}}),\n",
    "            apply_constraints_fn=self.apply_constraints_fn,\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "dm = load_data('dummy')\n",
    "model = load_ml_module('dummy')\n",
    "xs_train, ys_train = dm['train']\n",
    "xs_test, ys_test = dm['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "daa41197e13c450aa72695274898896f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "53c7e196eb454718897ea6f220120594",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validity:  0.99600005\n"
     ]
    }
   ],
   "source": [
    "vcf = VanillaCF()\n",
    "vcf.init_apply_fns()\n",
    "cf = vcf.generate_cf(xs_test[0], model.pred_fn)\n",
    "assert cf.shape == xs_test[0].shape\n",
    "\n",
    "partial_gen = partial(vcf.generate_cf, pred_fn=model.pred_fn)\n",
    "cfs = jax.vmap(partial_gen)(xs_test)\n",
    "\n",
    "print(\"Validity: \", keras.metrics.binary_accuracy(\n",
    "    (1 - model.pred_fn(xs_test)).round(),\n",
    "    model.pred_fn(cfs)\n",
    ").mean())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
