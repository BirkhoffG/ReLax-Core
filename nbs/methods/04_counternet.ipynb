{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CounterNet\n",
    "\n",
    "> A prediction-aware recourse model\n",
    "\n",
    "* Paper link: https://arxiv.org/abs/2109.07557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.counternet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import CFModule, ParametricCFModule\n",
    "from relax.base import BaseConfig\n",
    "from relax.utils import auto_reshaping, grad_update, validate_configs\n",
    "from relax.data_utils import Feature, FeaturesList\n",
    "from relax.ml_model import MLP, MLPBlock\n",
    "from relax.data_module import DataModule"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| exporti\n",
    "class CounterNetConfig(BaseConfig):\n",
    "    \"\"\"Configurator of `CounterNetModel`.\"\"\"\n",
    "\n",
    "    enc_sizes: List[int] = Field([50,10], description='Encoder sizes.')\n",
    "    pred_sizes: List[int] = Field([10], description='Predictor sizes.')\n",
    "    exp_sizes: List[int] = Field([50, 50], description='CF generator sizes.')\n",
    "    dropout_rate: float = Field(0.3, description='Dropout rate.')\n",
    "    lr: float = 0.003\n",
    "    lambda_1: float = 1.0\n",
    "    lambda_2: float = 0.2\n",
    "    lambda_3: float = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetModel(keras.Model):\n",
    "    \"\"\"CounterNet Model\"\"\"\n",
    "    def __init__(self, config: Dict | CounterNetConfig = None):\n",
    "        \"\"\"CounterNet model architecture.\"\"\"\n",
    "        super().__init__()\n",
    "        config = CounterNetConfig() if config is None else config\n",
    "        self.config: CounterNetConfig = validate_configs(config, CounterNetConfig)\n",
    "        self.dropout_rate = self.config.dropout_rate\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        self.encoder = keras.Sequential([\n",
    "            MLPBlock(size, self.dropout_rate) for size in self.config.enc_sizes\n",
    "        ])\n",
    "        # predictor\n",
    "        self.predictor = keras.Sequential([\n",
    "            MLPBlock(size, self.dropout_rate) for size in self.config.pred_sizes\n",
    "        ])\n",
    "        self.pred_linear = keras.layers.Dense(2, activation='softmax')\n",
    "        # explainer\n",
    "        self.explainer = MLP(\n",
    "            self.config.exp_sizes, output_size=input_shape[-1], \n",
    "            dropout_rate=self.dropout_rate, last_activation='linear'\n",
    "        )\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        z = self.encoder(x, training=training)\n",
    "        # predict y_hat\n",
    "        pred = self.predictor(z, training=training)\n",
    "        y_hat = self.pred_linear(pred)\n",
    "        # explain x\n",
    "        z_exp = keras.ops.concatenate([z, pred], axis=-1)\n",
    "        cfs = self.explainer(z_exp, training=training)\n",
    "        return pred, cfs\n",
    "    \n",
    "    def stateless_forward(self, params, xs, training=False):\n",
    "        (pred, cfs), non_trainable_params = self.stateless_call(\n",
    "            *params, xs, training=training\n",
    "        )\n",
    "        (cf_ys, _), non_trainable_params = self.stateless_call(\n",
    "            *params, cfs, training=training\n",
    "        )\n",
    "        return pred, cfs, cf_ys\n",
    "    \n",
    "    def pred_loss_fn(self, params, xs, ys, training=False):\n",
    "        (pred, cfs), non_trainable_params = self.stateless_call(\n",
    "            *params, xs, training=training\n",
    "        )\n",
    "        pred_loss = self.config.lambda_1 * self.loss_1(ys, pred)\n",
    "        return pred_loss\n",
    "    \n",
    "    def exp_loss_fn(self, params, xs, ys, training=False):\n",
    "        pred, cfs, cf_ys = self.forward(params, xs, training=training)\n",
    "        # TODO: Do we need the hard or soft labels?\n",
    "        y_prime = 1 - jnp.round(pred)\n",
    "        loss_2 = self.loss_2(y_prime, cf_ys)\n",
    "        loss_3 = self.loss_3(xs, cfs)\n",
    "        exp_loss = self.config.lambda_2 * loss_2 + self.config.lambda_3 * loss_3\n",
    "        return exp_loss\n",
    "    \n",
    "    def freeze_predictor(self, freeze=True):\n",
    "        freezing_layers = [self.encoder, self.predictor, self.pred_linear]\n",
    "        for layer in freezing_layers:\n",
    "            layer.trainable = not freeze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pred_step(params, batch, opt_state, model):\n",
    "    opt = model.opt_1\n",
    "    loss, grads = jax.value_and_grad(model.pred_loss_fn)(params, batch, training=True)\n",
    "    trainable_params, opt_state = opt.stateless_apply(\n",
    "        opt_state, grads, trainable_params\n",
    "    )\n",
    "    return (trainable_params, opt_state), loss\n",
    "\n",
    "\n",
    "def exp_step(params, batch, opt_state, model):\n",
    "    opt = model.opt_2\n",
    "    loss, grads = jax.value_and_grad(model.exp_loss_fn)(params, batch, training=True)\n",
    "    trainable_params, opt_state = opt.stateless_apply(\n",
    "        opt_state, grads, trainable_params\n",
    "    )\n",
    "    return (trainable_params, opt_state), loss\n",
    "\n",
    "def train_step(\n",
    "    state, \n",
    "    batch,\n",
    "    model: CounterNetModel,\n",
    "):\n",
    "    trainable_params, non_trainable_params, (opt_1_state, opt_2_state), metrics_vars = state\n",
    "    xs, ys = batch\n",
    "\n",
    "    # Stage 1: Train predictor\n",
    "    model.freeze_predictor(freeze=False)\n",
    "    (trainable_params, opt_1_state), loss = pred_step(\n",
    "        (trainable_params, non_trainable_params), (xs, ys), opt_1_state, model\n",
    "    )\n",
    "\n",
    "    # Stage 2: Train explainer\n",
    "    model.freeze_predictor(freeze=True)\n",
    "    (trainable_params, opt_2_state), loss = exp_step(\n",
    "        (trainable_params, non_trainable_params), (xs, ys), opt_2_state, model\n",
    "    )\n",
    "\n",
    "    # Update state\n",
    "    state = trainable_params, non_trainable_params, (opt_1_state, opt_2_state), metrics_vars\n",
    "    return state, loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No GPU/TPU found, falling back to CPU. (Set TF_CPP_MIN_LOG_LEVEL=0 and rerun for more info.)\n"
     ]
    }
   ],
   "source": [
    "cfnet = CounterNetModel()\n",
    "opt_1 = keras.optimizers.Adam(learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32),\n",
       " Array([[0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.],\n",
       "        [0., 0.]], dtype=float32))"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfnet(jnp.zeros((10, 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(2, 50), dtype=float32, name=variable>,\n",
       " <KerasVariable shape=(50,), dtype=float32, name=variable_1>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>,\n",
       " <KerasVariable shape=(50, 10), dtype=float32, name=variable_2>,\n",
       " <KerasVariable shape=(10,), dtype=float32, name=variable_3>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>,\n",
       " <KerasVariable shape=(10, 10), dtype=float32, name=variable_4>,\n",
       " <KerasVariable shape=(10,), dtype=float32, name=variable_5>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>,\n",
       " <KerasVariable shape=(10, 2), dtype=float32, name=variable_6>,\n",
       " <KerasVariable shape=(2,), dtype=float32, name=variable_7>,\n",
       " <KerasVariable shape=(20, 50), dtype=float32, name=variable_8>,\n",
       " <KerasVariable shape=(50,), dtype=float32, name=variable_9>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>,\n",
       " <KerasVariable shape=(50, 50), dtype=float32, name=variable_10>,\n",
       " <KerasVariable shape=(50,), dtype=float32, name=variable_11>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>,\n",
       " <KerasVariable shape=(50, 2), dtype=float32, name=variable_12>,\n",
       " <KerasVariable shape=(2,), dtype=float32, name=variable_13>]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfnet.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(20, 50), dtype=float32, name=variable_8>,\n",
       " <KerasVariable shape=(50,), dtype=float32, name=variable_9>,\n",
       " <KerasVariable shape=(50, 50), dtype=float32, name=variable_10>,\n",
       " <KerasVariable shape=(50,), dtype=float32, name=variable_11>,\n",
       " <KerasVariable shape=(50, 2), dtype=float32, name=variable_12>,\n",
       " <KerasVariable shape=(2,), dtype=float32, name=variable_13>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfnet.freeze_predictor(freeze=True)\n",
    "cfnet.trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<KerasVariable shape=(2, 50), dtype=float32, name=variable_14>,\n",
       " <KerasVariable shape=(50,), dtype=float32, name=variable_15>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>,\n",
       " <KerasVariable shape=(50, 10), dtype=float32, name=variable_16>,\n",
       " <KerasVariable shape=(10,), dtype=float32, name=variable_17>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>,\n",
       " <KerasVariable shape=(10, 10), dtype=float32, name=variable_18>,\n",
       " <KerasVariable shape=(10,), dtype=float32, name=variable_19>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>,\n",
       " <KerasVariable shape=(10, 2), dtype=float32, name=variable_20>,\n",
       " <KerasVariable shape=(2,), dtype=float32, name=variable_21>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>,\n",
       " <KerasVariable shape=(2,), dtype=uint32, name=seed_generator_state>]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfnet.non_trainable_variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "opt_1.build(cfnet.trainable_variables)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Training Module\n",
    "\n",
    "Define the `CounterNetTrainingModule` for training `CounterNetModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def partition_trainable_params(params: hk.Params, trainable_name: str):\n",
    "    trainable_params, non_trainable_params = hk.data_structures.partition(\n",
    "        lambda m, n, p: trainable_name in m, params\n",
    "    )\n",
    "    return trainable_params, non_trainable_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def project_immutable_features(x, cf: jax.Array, imutable_idx_list: List[int]):\n",
    "    cf = cf.at[:, imutable_idx_list].set(x[:, imutable_idx_list])\n",
    "    return cf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetTrainingModuleConfigs(BaseParser):\n",
    "    lr: float = 0.003\n",
    "    lambda_1: float = 1.0\n",
    "    lambda_2: float = 0.2\n",
    "    lambda_3: float = 0.1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetTrainingModule(BaseTrainingModule):\n",
    "    _data_module: TabularDataModule\n",
    "\n",
    "    def __init__(self, m_configs: Dict[str, Any]):\n",
    "        self.save_hyperparameters(m_configs)\n",
    "        self.net = make_model(m_configs, CounterNetModel)\n",
    "        self.configs = validate_configs(m_configs, CounterNetTrainingModuleConfigs)\n",
    "        # self.configs = CounterNetTrainingModuleConfigs(**m_configs)\n",
    "        self.opt_1 = optax.adam(learning_rate=self.configs.lr)\n",
    "        self.opt_2 = optax.adam(learning_rate=self.configs.lr)\n",
    "\n",
    "    def init_net_opt(self, data_module: TabularDataModule, key):\n",
    "        # hook data_module\n",
    "        self._data_module = data_module\n",
    "        X, _ = data_module.train_dataset[:]\n",
    "\n",
    "        # manually init multiple opts\n",
    "        params, opt_1_state = init_net_opt(\n",
    "            self.net, self.opt_1, X=X[:100], key=key\n",
    "        )\n",
    "        trainable_params, _ = partition_trainable_params(\n",
    "            params, trainable_name=\"counter_net_model/Explainer\"\n",
    "        )\n",
    "        opt_2_state = self.opt_2.init(trainable_params)\n",
    "        return params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\", \"is_training\"])\n",
    "    def forward(self, params, rng_key, x, is_training: bool = True):\n",
    "        # first forward to get y_pred and normalized cf\n",
    "        y_pred, cf = self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "        cf = self._data_module.apply_constraints(x, cf, hard=not is_training)\n",
    "\n",
    "        # second forward to calulate cf_y\n",
    "        cf_y, _ = self.net.apply(params, rng_key, cf, is_training=is_training)\n",
    "        return y_pred, cf, cf_y\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def predict(self, params, rng_key, x):\n",
    "        y_pred, _ = self.net.apply(params, rng_key, x, is_training=False)\n",
    "        return y_pred\n",
    "\n",
    "    def generate_cfs(self, X: Array, params, rng_key) -> chex.ArrayBatched:\n",
    "        y_pred, cfs = self.net.apply(params, rng_key, X, is_training=False)\n",
    "        # cfs = cfs + X\n",
    "        cfs = self._data_module.apply_constraints(X, cfs, hard=True)\n",
    "        return cfs\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def loss_fn_1(self, y_pred, y):\n",
    "        return jnp.mean(vmap(optax.l2_loss)(y_pred, y))\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def loss_fn_2(self, cf_y, y_prime):\n",
    "        return jnp.mean(vmap(optax.l2_loss)(cf_y, y_prime))\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def loss_fn_3(self, x, cf):\n",
    "        return jnp.mean(vmap(optax.l2_loss)(x, cf))\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\", \"is_training\"])\n",
    "    def pred_loss_fn(self, params, rng_key, batch, is_training: bool = True):\n",
    "        x, y = batch\n",
    "        y_pred, cf = self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "        return self.configs.lambda_1 * self.loss_fn_1(y_pred, y)\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\", \"is_training\"])\n",
    "    def exp_loss_fn(\n",
    "        self,\n",
    "        trainable_params,\n",
    "        non_trainable_params,\n",
    "        rng_key,\n",
    "        batch,\n",
    "        is_training: bool = True,\n",
    "    ):\n",
    "        # merge trainable and non_trainable params\n",
    "        params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "        x, y = batch\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training=is_training)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "        loss_2, loss_3 = self.loss_fn_2(cf_y, y_prime), self.loss_fn_3(x, cf)\n",
    "        return self.configs.lambda_2 * loss_2 + self.configs.lambda_3 * loss_3\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\",])\n",
    "    def _predictor_step(self, params, opt_state, rng_key, batch):\n",
    "        grads = jax.grad(self.pred_loss_fn)(params, rng_key, batch)\n",
    "        upt_params, opt_state = grad_update(grads, params, opt_state, self.opt_1)\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\",])\n",
    "    def _explainer_step(self, params, opt_state, rng_key, batch):\n",
    "        trainable_params, non_trainable_params = partition_trainable_params(\n",
    "            params, trainable_name=\"counter_net_model/Explainer\"\n",
    "        )\n",
    "        grads = jax.grad(self.exp_loss_fn)(\n",
    "            trainable_params, non_trainable_params, rng_key, batch\n",
    "        )\n",
    "        upt_trainable_params, opt_state = grad_update(\n",
    "            grads, trainable_params, opt_state, self.opt_2\n",
    "        )\n",
    "        upt_params = hk.data_structures.merge(\n",
    "            upt_trainable_params, non_trainable_params\n",
    "        )\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def _training_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opts_state: Tuple[optax.GradientTransformation, optax.GradientTransformation],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ):\n",
    "        opt_1_state, opt_2_state = opts_state\n",
    "        params, opt_1_state = self._predictor_step(params, opt_1_state, rng_key, batch)\n",
    "        upt_params, opt_2_state = self._explainer_step(\n",
    "            params, opt_2_state, rng_key, batch\n",
    "        )\n",
    "        return upt_params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    def _training_step_logs(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training=False)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "\n",
    "        loss_1, loss_2, loss_3 = (\n",
    "            self.loss_fn_1(y_pred, y),\n",
    "            self.loss_fn_2(cf_y, y_prime),\n",
    "            self.loss_fn_3(x, cf),\n",
    "        )\n",
    "        logs = {\n",
    "            \"train/train_loss_1\": loss_1.item(),\n",
    "            \"train/train_loss_2\": loss_2.item(),\n",
    "            \"train/train_loss_3\": loss_3.item(),\n",
    "        }\n",
    "        return logs\n",
    "\n",
    "    def training_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opts_state: Tuple[optax.OptState, optax.OptState],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ) -> Tuple[hk.Params, Tuple[optax.OptState, optax.OptState]]:\n",
    "        upt_params, (opt_1_state, opt_2_state) = self._training_step(\n",
    "            params, opts_state, rng_key, batch\n",
    "        )\n",
    "\n",
    "        logs = self._training_step_logs(upt_params, rng_key, batch)\n",
    "        self.log_dict(logs)\n",
    "        return upt_params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    def validation_step(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training=False)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "\n",
    "        loss_1, loss_2, loss_3 = (\n",
    "            self.loss_fn_1(y_pred, y),\n",
    "            self.loss_fn_2(cf_y, y_prime),\n",
    "            self.loss_fn_3(x, cf),\n",
    "        )\n",
    "        loss_1, loss_2, loss_3 = map(np.asarray, (loss_1, loss_2, loss_3))\n",
    "        logs = {\n",
    "            \"val/accuracy\": accuracy(y, y_pred),\n",
    "            \"val/validity\": accuracy(cf_y, y_prime),\n",
    "            \"val/proximity\": proximity(x, cf),\n",
    "            \"val/val_loss_1\": loss_1,\n",
    "            \"val/val_loss_2\": loss_2,\n",
    "            \"val/val_loss_3\": loss_3,\n",
    "            \"val/val_loss\": loss_1 + loss_2 + loss_3,\n",
    "        }\n",
    "        self.log_dict(logs)\n",
    "        return logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Explanation Module\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CounterNet architecture](./images/CounterNet-architecture.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CounterNet` consists of three objectives:\n",
    "\n",
    "1. **predictive accuracy**: the predictor network should output accurate predictions $\\hat{y}_x$; \n",
    "2. **counterfactual validity**: CF examples $x'$ produced by the CF generator network should be valid (e.g. $\\hat{y}_{x} + \\hat{y}_{x'}=1$);\n",
    "3. **minimizing cost of change**: minimal modifications should be required to change input instance $x$ to CF example $x'$.\n",
    "\n",
    "The objective function of `CounterNet`:\n",
    "\n",
    "$$\n",
    "\\operatorname*{argmin}_{\\mathbf{\\theta}} \\frac{1}{N}\\sum\\nolimits_{i=1}^{N} \n",
    "    \\bigg[ \n",
    "    \\lambda_1 \\cdot \\! \\underbrace{\\left(y_i- \\hat{y}_{x_i}\\right)^2}_{\\text{Prediction Loss}\\ (\\mathcal{L}_1)} + \n",
    "    \\;\\lambda_2 \\cdot \\;\\; \\underbrace{\\left(\\hat{y}_{x_i}- \\left(1 - \\hat{y}_{x_i'}\\right)\\right)^2}_{\\text{Validity Loss}\\ (\\mathcal{L}_2)} \\,+ \n",
    "    \\;\\lambda_3 \\cdot \\!\\! \\underbrace{\\left(x_i- x'_i\\right)^2}_{\\text{Cost of change Loss}\\ (\\mathcal{L}_3)}\n",
    "    \\bigg]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CounterNet` applies two-stage gradient updates to `CounterNetModel` \n",
    "for each `training_step` (see `CounterNetTrainingModule`).\n",
    "\n",
    "1. The first gradient update optimizes for predictive accuracy: \n",
    "$\\theta^{(1)} = \\theta^{(0)} - \\nabla_{\\theta^{(0)}} (\\lambda_1 \\cdot \\mathcal{L}_1)$.\n",
    "2. The second gradient update optimizes for generating CF explanation:\n",
    "$\\theta^{(2)}_g = \\theta^{(1)}_g - \\nabla_{\\theta^{(1)}_g} (\\mathcal \\lambda_2 \\cdot \\mathcal{L}_2 + \\lambda_3 \\cdot \\mathcal{L}_3)$\n",
    "\n",
    "The design choice of this optimizing procedure is made due to *improved convergence of the model*,\n",
    "and *improved adversarial robustness of the predictor network*. \n",
    "The [CounterNet paper](https://arxiv.org/abs/2109.07557) elaborates the design choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetConfigs(CounterNetTrainingModuleConfigs, CounterNetModelConfigs):\n",
    "    \"\"\"Configurator of `CounterNet`.\"\"\"\n",
    "\n",
    "    enc_sizes: List[int] = Field(\n",
    "        [50,10], description=\"Sequence of layer sizes for encoder network.\"\n",
    "    )\n",
    "    dec_sizes: List[int] = Field(\n",
    "        [10], description=\"Sequence of layer sizes for predictor.\"\n",
    "    ) \n",
    "    exp_sizes: List[int] = Field(\n",
    "        [50, 50], description=\"Sequence of layer sizes for CF generator.\"\n",
    "    )\n",
    "    \n",
    "    dropout_rate: float = Field(\n",
    "        0.3, description=\"Dropout rate.\"\n",
    "    )\n",
    "    lr: float = Field(\n",
    "        0.003, description=\"Learning rate for training `CounterNet`.\"\n",
    "    ) \n",
    "    lambda_1: float = Field(\n",
    "        1.0, description=\" $\\lambda_1$ for balancing the prediction loss $\\mathcal{L}_1$.\"\n",
    "    ) \n",
    "    lambda_2: float = Field(\n",
    "        0.2, description=\" $\\lambda_2$ for balancing the prediction loss $\\mathcal{L}_2$.\"\n",
    "    ) \n",
    "    lambda_3: float = Field(\n",
    "        0.1, description=\" $\\lambda_3$ for balancing the prediction loss $\\mathcal{L}_3$.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNet(BaseCFModule, BaseParametricCFModule, BasePredFnCFModule):\n",
    "    \"\"\"API for CounterNet Explanation Module.\"\"\"\n",
    "    params: hk.Params = None\n",
    "    module: CounterNetTrainingModule\n",
    "    name: str = 'CounterNet'\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        m_configs: dict | CounterNetConfigs = None # configurator of hyperparamters; see `CounterNetConfigs`\n",
    "    ):\n",
    "        if m_configs is None:\n",
    "            m_configs = CounterNetConfigs()\n",
    "        self.module = CounterNetTrainingModule(m_configs)\n",
    "\n",
    "    def _is_module_trained(self):\n",
    "        return not (self.params is None)\n",
    "    \n",
    "    def train(\n",
    "        self, \n",
    "        datamodule: TabularDataModule, # data module\n",
    "        t_configs: TrainingConfigs | dict = None, # training configs\n",
    "        *args, **kwargs\n",
    "    ):\n",
    "        _default_t_configs = dict(\n",
    "            n_epochs=100, batch_size=128\n",
    "        )\n",
    "        if t_configs is None: t_configs = _default_t_configs\n",
    "        params, _ = train_model(self.module, datamodule, t_configs)\n",
    "        self.params = params\n",
    "\n",
    "    @auto_reshaping('x')\n",
    "    def generate_cf(self, x: jnp.ndarray, pred_fn = None) -> jnp.ndarray:\n",
    "        return self.module.generate_cfs(x, self.params, rng_key=jax.random.PRNGKey(0))\n",
    "\n",
    "    def generate_cfs(self, X: jnp.ndarray, pred_fn = None) -> jnp.ndarray:\n",
    "        return self.module.generate_cfs(X, self.params, rng_key=jax.random.PRNGKey(0))\n",
    "    \n",
    "    def pred_fn(self, X: jax.Array):\n",
    "        rng_key = jax.random.PRNGKey(0)\n",
    "        y_pred = self.module.predict(self.params, rng_key, X)\n",
    "        return y_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic usage of `CounterNet`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.data import load_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/sklearn/preprocessing/_encoders.py:828: FutureWarning: `sparse` was renamed to `sparse_output` in version 1.2 and will be removed in 1.4. `sparse_output` is ignored unless you leave `sparse` to its default value.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dm = load_data(\"adult\", data_configs=dict(sample_frac=0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `CounterNet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counternet = CounterNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(counternet, BaseParametricCFModule)\n",
    "assert isinstance(counternet, BaseCFModule)\n",
    "assert isinstance(counternet, BasePredFnCFModule)\n",
    "assert hasattr(counternet, 'pred_fn')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/chuck/opt/anaconda3/envs/relax/lib/python3.8/site-packages/relax/_ckpt_manager.py:47: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 20/20 [00:05<00:00,  3.62batch/s, train/train_loss_1=0.0568, train/train_loss_2=0.221, train/train_loss_3=0.117]\n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "t_configs = dict(n_epochs=1, batch_size=128)\n",
    "counternet.train(dm, t_configs=t_configs)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = dm.test_dataset[:]\n",
    "y_pred = counternet.pred_fn(X)\n",
    "assert y_pred.shape == (len(y), 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a CF explanation for a given `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x, _ = dm.test_dataset[0]\n",
    "cf = counternet.generate_cf(x)\n",
    "assert x.shape == cf.shape\n",
    "assert cf.shape == (29,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate CF explanations for given `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, _ = dm.test_dataset[:]\n",
    "cfs = counternet.generate_cfs(X)\n",
    "assert X.shape == cfs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
