{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CounterNet\n",
    "\n",
    "> A prediction-aware recourse model\n",
    "\n",
    "* Paper link: https://arxiv.org/abs/2109.07557"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.counternet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import CFModule, ParametricCFModule\n",
    "from relax.base import BaseConfig, PredFnMixedin\n",
    "from relax.utils import auto_reshaping, grad_update, validate_configs\n",
    "from relax.data_utils import Feature, FeaturesList\n",
    "from relax.data_module import DataModule\n",
    "from relax.evaluate import compute_proximity, PredictiveAccuracy\n",
    "from relax.methods.base import ParametricCFModule\n",
    "# Legacy code for making haiku works\n",
    "import haiku as hk\n",
    "from relax.legacy.utils import make_hk_module, init_net_opt\n",
    "from relax.legacy.module import MLP, BaseTrainingModule\n",
    "from relax.legacy.trainer import train_model"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetModel(hk.Module):\n",
    "    \"\"\"CounterNet Model\"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        enc_sizes: list,\n",
    "        dec_sizes: list,\n",
    "        exp_sizes: list,\n",
    "        dropout_rate: float,\n",
    "        name: str = None,  # Name of the module.\n",
    "    ):\n",
    "        \"\"\"CounterNet model architecture.\"\"\"\n",
    "        super().__init__(name=name)\n",
    "        self.enc_sizes = enc_sizes\n",
    "        self.dec_sizes = dec_sizes\n",
    "        self.exp_sizes = exp_sizes\n",
    "        self.dropout_rate = dropout_rate\n",
    "\n",
    "    def __call__(self, x: jnp.ndarray, is_training: bool = True) -> jnp.ndarray:\n",
    "        input_shape = x.shape[-1]\n",
    "        # encoder\n",
    "        z = MLP(self.enc_sizes, self.dropout_rate, name=\"Encoder\")(\n",
    "            x, is_training\n",
    "        )\n",
    "\n",
    "        # prediction\n",
    "        pred = MLP(self.dec_sizes, self.dropout_rate, name=\"Predictor\")(\n",
    "            z, is_training\n",
    "        )\n",
    "        y_hat = hk.Linear(1, name=\"Predictor\")(pred)\n",
    "        y_hat = jax.nn.sigmoid(y_hat)\n",
    "\n",
    "        # explain\n",
    "        z_exp = jnp.concatenate((z, pred), axis=-1)\n",
    "        cf = MLP(self.exp_sizes, self.dropout_rate, name=\"Explainer\")(\n",
    "            z_exp, is_training\n",
    "        )\n",
    "        cf = hk.Linear(input_shape, name=\"Explainer\")(cf)\n",
    "        return y_hat, cf\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Training Module\n",
    "\n",
    "Define the `CounterNetTrainingModule` for training `CounterNetModel`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export \n",
    "def partition_trainable_params(params: hk.Params, trainable_name: str):\n",
    "    trainable_params, non_trainable_params = hk.data_structures.partition(\n",
    "        lambda m, n, p: trainable_name in m, params\n",
    "    )\n",
    "    return trainable_params, non_trainable_params\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetTrainingModule(BaseTrainingModule):\n",
    "    def __init__(self, config: CounterNetConfig | dict):\n",
    "        self.save_hyperparameters(config.dict())\n",
    "        self.configs = validate_configs(config, CounterNetConfig)\n",
    "        self.net = make_hk_module(\n",
    "            CounterNetModel,\n",
    "            enc_sizes=config.enc_sizes,\n",
    "            dec_sizes=config.pred_sizes,\n",
    "            exp_sizes=config.exp_sizes,\n",
    "            dropout_rate=config.dropout_rate\n",
    "        )\n",
    "        \n",
    "        self.opt_1 = optax.adam(learning_rate=config.lr)\n",
    "        self.opt_2 = optax.adam(learning_rate=config.lr)\n",
    "\n",
    "    def init_net_opt(self, data_module: DataModule, key):\n",
    "        # hook data_module\n",
    "        self._data_module = data_module\n",
    "        X, _ = data_module.sample(128)\n",
    "        rng_key, key = random.split(key)\n",
    "\n",
    "        # manually init multiple opts\n",
    "        params, opt_1_state = init_net_opt(\n",
    "            self.net, self.opt_1, X=X, key=rng_key\n",
    "        )\n",
    "        trainable_params, _ = partition_trainable_params(\n",
    "            params, trainable_name=\"counter_net_model/Explainer\"\n",
    "        )\n",
    "        opt_2_state = self.opt_2.init(trainable_params)\n",
    "        return params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\", \"is_training\"])\n",
    "    def forward(self, params, rng_key, x, is_training: bool = True):\n",
    "        # first forward to get y_pred and normalized cf\n",
    "        y_pred, cf = self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "        cf = self._data_module.apply_constraints(x, cf, hard=not is_training)\n",
    "\n",
    "        # second forward to calulate cf_y\n",
    "        cf_y, _ = self.net.apply(params, rng_key, cf, is_training=is_training)\n",
    "        return y_pred, cf, cf_y\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def pred_fn(self, params, rng_key, xs):\n",
    "        y_pred, _ = self.net.apply(params, rng_key, xs, is_training=False)\n",
    "        return y_pred\n",
    "    \n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def generate_cf(self, params, rng_key, x):\n",
    "        y_pred, cf = self.net.apply(params, rng_key, x, is_training=False)\n",
    "        cf = self._data_module.apply_constraints(x, cf, hard=True)\n",
    "        return cf\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def loss_fn_1(self, y_pred, y):\n",
    "        return jnp.mean(vmap(optax.l2_loss)(y_pred, y))\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def loss_fn_2(self, cf_y, y_prime):\n",
    "        return jnp.mean(vmap(optax.l2_loss)(cf_y, y_prime))\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def loss_fn_3(self, x, cf):\n",
    "        return jnp.mean(vmap(optax.l2_loss)(x, cf))\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\", \"is_training\"])\n",
    "    def pred_loss_fn(self, params, rng_key, batch, is_training: bool = True):\n",
    "        x, y = batch\n",
    "        y_pred, cf = self.net.apply(params, rng_key, x, is_training=is_training)\n",
    "        return self.configs.lambda_1 * self.loss_fn_1(y_pred, y)\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\", \"is_training\"])\n",
    "    def exp_loss_fn(\n",
    "        self,\n",
    "        trainable_params,\n",
    "        non_trainable_params,\n",
    "        rng_key,\n",
    "        batch,\n",
    "        is_training: bool = True,\n",
    "    ):\n",
    "        # merge trainable and non_trainable params\n",
    "        params = hk.data_structures.merge(trainable_params, non_trainable_params)\n",
    "        x, y = batch\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training=is_training)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "        loss_2, loss_3 = self.loss_fn_2(cf_y, y_prime), self.loss_fn_3(x, cf)\n",
    "        return self.configs.lambda_2 * loss_2 + self.configs.lambda_3 * loss_3\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\",])\n",
    "    def _predictor_step(self, params, opt_state, rng_key, batch):\n",
    "        grads = jax.grad(self.pred_loss_fn)(params, rng_key, batch)\n",
    "        upt_params, opt_state = grad_update(grads, params, opt_state, self.opt_1)\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\",])\n",
    "    def _explainer_step(self, params, opt_state, rng_key, batch):\n",
    "        trainable_params, non_trainable_params = partition_trainable_params(\n",
    "            params, trainable_name=\"counter_net_model/Explainer\"\n",
    "        )\n",
    "        grads = jax.grad(self.exp_loss_fn)(\n",
    "            trainable_params, non_trainable_params, rng_key, batch\n",
    "        )\n",
    "        upt_trainable_params, opt_state = grad_update(\n",
    "            grads, trainable_params, opt_state, self.opt_2\n",
    "        )\n",
    "        upt_params = hk.data_structures.merge(\n",
    "            upt_trainable_params, non_trainable_params\n",
    "        )\n",
    "        return upt_params, opt_state\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def _training_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opts_state: Tuple[optax.OptState, optax.OptState],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[Array, Array],\n",
    "    ):\n",
    "        opt_1_state, opt_2_state = opts_state\n",
    "        params, opt_1_state = self._predictor_step(params, opt_1_state, rng_key, batch)\n",
    "        upt_params, opt_2_state = self._explainer_step(\n",
    "            params, opt_2_state, rng_key, batch\n",
    "        )\n",
    "        return upt_params, (opt_1_state, opt_2_state)\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def _training_step_logs(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training=False)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "\n",
    "        loss_1, loss_2, loss_3 = (\n",
    "            self.loss_fn_1(y_pred, y),\n",
    "            self.loss_fn_2(cf_y, y_prime),\n",
    "            self.loss_fn_3(x, cf),\n",
    "        )\n",
    "        logs = {\n",
    "            \"train/train_loss_1\": loss_1,#.item(),\n",
    "            \"train/train_loss_2\": loss_2,#.item(),\n",
    "            \"train/train_loss_3\": loss_3,#.item(),\n",
    "        }\n",
    "        return logs\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def training_step(\n",
    "        self,\n",
    "        params: hk.Params,\n",
    "        opts_state: Tuple[optax.OptState, optax.OptState],\n",
    "        rng_key: random.PRNGKey,\n",
    "        batch: Tuple[jnp.array, jnp.array],\n",
    "    ) -> Tuple[hk.Params, Tuple[optax.OptState, optax.OptState]]:\n",
    "        upt_params, (opt_1_state, opt_2_state) = self._training_step(\n",
    "            params, opts_state, rng_key, batch\n",
    "        )\n",
    "\n",
    "        logs = self._training_step_logs(upt_params, rng_key, batch)\n",
    "        return logs, (upt_params, (opt_1_state, opt_2_state))\n",
    "\n",
    "    @partial(jax.jit, static_argnames=[\"self\"])\n",
    "    def validation_step(self, params, rng_key, batch):\n",
    "        x, y = batch\n",
    "        y_pred, cf, cf_y = self.forward(params, rng_key, x, is_training=False)\n",
    "        y_prime = 1 - jnp.round(y_pred)\n",
    "\n",
    "        loss_1, loss_2, loss_3 = (\n",
    "            self.loss_fn_1(y_pred, y),\n",
    "            self.loss_fn_2(cf_y, y_prime),\n",
    "            self.loss_fn_3(x, cf),\n",
    "        )\n",
    "        # loss_1, loss_2, loss_3 = map(np.asarray, (loss_1, loss_2, loss_3))\n",
    "        logs = {\n",
    "            # \"val/accuracy\": accuracy(y, y_pred),\n",
    "            # \"val/validity\": accuracy(cf_y, y_prime),\n",
    "            # \"val/proximity\": compute_proximity(x, cf),\n",
    "            \"val/val_loss_1\": loss_1,\n",
    "            \"val/val_loss_2\": loss_2,\n",
    "            \"val/val_loss_3\": loss_3,\n",
    "            \"val/val_loss\": loss_1 + loss_2 + loss_3,\n",
    "        }\n",
    "        return logs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CounterNet Explanation Module\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CounterNet architecture](./images/CounterNet-architecture.svg)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CounterNet` consists of three objectives:\n",
    "\n",
    "1. **predictive accuracy**: the predictor network should output accurate predictions $\\hat{y}_x$; \n",
    "2. **counterfactual validity**: CF examples $x'$ produced by the CF generator network should be valid (e.g. $\\hat{y}_{x} + \\hat{y}_{x'}=1$);\n",
    "3. **minimizing cost of change**: minimal modifications should be required to change input instance $x$ to CF example $x'$.\n",
    "\n",
    "The objective function of `CounterNet`:\n",
    "\n",
    "$$\n",
    "\\operatorname*{argmin}_{\\mathbf{\\theta}} \\frac{1}{N}\\sum\\nolimits_{i=1}^{N} \n",
    "    \\bigg[ \n",
    "    \\lambda_1 \\cdot \\! \\underbrace{\\left(y_i- \\hat{y}_{x_i}\\right)^2}_{\\text{Prediction Loss}\\ (\\mathcal{L}_1)} + \n",
    "    \\;\\lambda_2 \\cdot \\;\\; \\underbrace{\\left(\\hat{y}_{x_i}- \\left(1 - \\hat{y}_{x_i'}\\right)\\right)^2}_{\\text{Validity Loss}\\ (\\mathcal{L}_2)} \\,+ \n",
    "    \\;\\lambda_3 \\cdot \\!\\! \\underbrace{\\left(x_i- x'_i\\right)^2}_{\\text{Cost of change Loss}\\ (\\mathcal{L}_3)}\n",
    "    \\bigg]\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`CounterNet` applies two-stage gradient updates to `CounterNetModel` \n",
    "for each `training_step` (see `CounterNetTrainingModule`).\n",
    "\n",
    "1. The first gradient update optimizes for predictive accuracy: \n",
    "$\\theta^{(1)} = \\theta^{(0)} - \\nabla_{\\theta^{(0)}} (\\lambda_1 \\cdot \\mathcal{L}_1)$.\n",
    "2. The second gradient update optimizes for generating CF explanation:\n",
    "$\\theta^{(2)}_g = \\theta^{(1)}_g - \\nabla_{\\theta^{(1)}_g} (\\mathcal \\lambda_2 \\cdot \\mathcal{L}_2 + \\lambda_3 \\cdot \\mathcal{L}_3)$\n",
    "\n",
    "The design choice of this optimizing procedure is made due to *improved convergence of the model*,\n",
    "and *improved adversarial robustness of the predictor network*. \n",
    "The [CounterNet paper](https://arxiv.org/abs/2109.07557) elaborates the design choices.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNetConfig(BaseConfig):\n",
    "    \"\"\"Configurator of `CounterNet`.\"\"\"\n",
    "\n",
    "    enc_sizes: List[int] = Field(\n",
    "        [50,10], description=\"Sequence of layer sizes for encoder network.\"\n",
    "    )\n",
    "    pred_sizes: List[int] = Field(\n",
    "        [10], description=\"Sequence of layer sizes for predictor.\"\n",
    "    ) \n",
    "    exp_sizes: List[int] = Field(\n",
    "        [50, 50], description=\"Sequence of layer sizes for CF generator.\"\n",
    "    )\n",
    "    \n",
    "    dropout_rate: float = Field(\n",
    "        0.3, description=\"Dropout rate.\"\n",
    "    )\n",
    "    lr: float = Field(\n",
    "        0.003, description=\"Learning rate for training `CounterNet`.\"\n",
    "    ) \n",
    "    lambda_1: float = Field(\n",
    "        1.0, description=\" $\\lambda_1$ for balancing the prediction loss $\\mathcal{L}_1$.\"\n",
    "    ) \n",
    "    lambda_2: float = Field(\n",
    "        0.2, description=\" $\\lambda_2$ for balancing the prediction loss $\\mathcal{L}_2$.\"\n",
    "    ) \n",
    "    lambda_3: float = Field(\n",
    "        0.1, description=\" $\\lambda_3$ for balancing the prediction loss $\\mathcal{L}_3$.\"\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class CounterNet(ParametricCFModule, PredFnMixedin):\n",
    "    \"\"\"API for CounterNet Explanation Module.\"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self, \n",
    "        config: dict | CounterNetConfig = None,\n",
    "        cfnet_module: CounterNetTrainingModule = None, \n",
    "        name: str = None\n",
    "    ):\n",
    "        if config is None:\n",
    "            config = CounterNetConfig()\n",
    "        config = validate_configs(config, CounterNetConfig)\n",
    "        name = \"CounterNet\" if name is None else name\n",
    "        self.module = cfnet_module\n",
    "        self._is_trained = False\n",
    "        super().__init__(config, name=name)\n",
    "\n",
    "    def _init_model(self, config: CounterNetConfig, cfnet_module: CounterNetTrainingModule):\n",
    "        if cfnet_module is None:\n",
    "            cfnet_module = CounterNetTrainingModule(config)\n",
    "        return cfnet_module\n",
    "    \n",
    "    @property\n",
    "    def is_trained(self) -> bool:\n",
    "        return self._is_trained\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, # data module\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **kwargs\n",
    "    ):\n",
    "        self.module = self._init_model(self.config, self.module)\n",
    "        self.params, _ = train_model(\n",
    "            self.module, data, batch_size=batch_size, epochs=epochs, **kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        return self\n",
    "\n",
    "    @auto_reshaping('x')\n",
    "    def generate_cf(self, x: jax.Array, rng_key=jrand.PRNGKey(0), **kwargs) -> jax.Array:\n",
    "        return self.module.generate_cf(self.params, rng_key=rng_key, x=x)\n",
    "    \n",
    "    def pred_fn(self, xs: jax.Array):\n",
    "        y_pred = self.module.pred_fn(self.params, rng_key=jrand.PRNGKey(0), xs=xs)\n",
    "        return y_pred\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Basic usage of `CounterNet`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from relax.data_module import load_data\n",
    "from copy import deepcopy\n",
    "import chex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dm = load_data(\"adult\", data_configs=dict(sample_frac=0.1))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define `CounterNet`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counternet = CounterNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert isinstance(counternet, ParametricCFModule)\n",
    "assert isinstance(counternet, CFModule)\n",
    "assert isinstance(counternet, PredFnMixedin)\n",
    "assert hasattr(counternet, 'pred_fn')\n",
    "assert counternet.module is None\n",
    "assert counternet.is_trained is False\n",
    "assert not hasattr(counternet, 'params')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train the model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/birk/code/ReLax-Core/nbs/methods/../../relax/legacy/ckpt_manager.py:47: UserWarning: `monitor_metrics` is not specified in `CheckpointManager`. No checkpoints will be stored.\n",
      "  warnings.warn(\n",
      "Epoch 0: 100%|██████████| 191/191 [00:06<00:00, 27.77batch/s, train/train_loss_1=0.050380006, train/train_loss_2=0.07136493, train/train_loss_3=0.0991796]    \n"
     ]
    }
   ],
   "source": [
    "#| output: false\n",
    "counternet.train(dm, epochs=1, batch_size=128)\n",
    "assert counternet.is_trained is True\n",
    "assert hasattr(counternet, 'params')\n",
    "params = deepcopy(counternet.params)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predict labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs, y = dm['test']\n",
    "y_pred = counternet.pred_fn(xs)\n",
    "assert y_pred.shape == (len(y), 1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a CF explanation for a given `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cf = counternet.generate_cf(xs[0])\n",
    "assert xs[0].shape == cf.shape\n",
    "assert cf.shape == (29,)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate CF explanations for given `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfs = vmap(counternet.generate_cf)(xs)\n",
    "assert xs.shape == cfs.shape"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
