{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2C\n",
    "\n",
    "https://arxiv.org/abs/2209.13446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.l2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import ParametricCFModule\n",
    "from relax.base import BaseConfig\n",
    "from relax.utils import *\n",
    "from relax.data_utils import Feature, FeaturesList\n",
    "from relax.ml_model import MLP, MLPBlock\n",
    "from relax.data_module import DataModule\n",
    "from keras_core.random import SeedGenerator\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gumbel_softmax(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    logits: Array, # Logits for each class. Shape (batch_size, num_classes)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "):\n",
    "    \"\"\"The Gumbel softmax function.\"\"\"\n",
    "\n",
    "    gumbel_noise = jrand.gumbel(key, shape=logits.shape)\n",
    "    y = logits + gumbel_noise\n",
    "    return jax.nn.softmax(y / tau, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_categorical(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    logits: Array, # Logits for each class. Shape (batch_size, num_classes)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "    training: bool = True, # Apply gumbel softmax if training\n",
    "):\n",
    "    \"\"\"Sample from a categorical distribution.\"\"\"\n",
    "\n",
    "    def sample_cat(key, logits):\n",
    "        cat = jrand.categorical(key, logits=logits, axis=-1)\n",
    "        return jax.nn.one_hot(cat, logits.shape[-1])\n",
    "\n",
    "    return lax.cond(\n",
    "        training,\n",
    "        lambda _: gumbel_softmax(key, logits, tau=tau),\n",
    "        lambda _: sample_cat(key, logits),\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = jnp.array([[2.0, 1.0, 0.1], [1.0, 2.0, 3.0]])\n",
    "key = jrand.PRNGKey(0)\n",
    "output = sample_categorical(key, logits, tau=0.5, training=True)\n",
    "assert output.shape == logits.shape\n",
    "assert jnp.allclose(output.sum(axis=-1), 1.0)\n",
    "# low temperature -> one-hot\n",
    "output = sample_categorical(key, logits, tau=0.01, training=True)\n",
    "assert jnp.array_equal(\n",
    "    output.argmax(axis=-1), logits.argmax(axis=-1)\n",
    ")\n",
    "# high temperature -> uniform\n",
    "output = sample_categorical(key, logits, tau=100, training=True)\n",
    "assert jnp.max(output) - jnp.min(output) < 0.5\n",
    "\n",
    "output = sample_categorical(key, logits, tau=0.5, training=False)\n",
    "assert output.shape == logits.shape\n",
    "assert jnp.array_equal(\n",
    "    output.argmax(axis=-1), logits.argmax(axis=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_bernouli(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    prob: Array, # Logits for each class. Shape (batch_size, 1)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "    training: bool = True, # Apply gumbel softmax if training\n",
    ") -> Array:\n",
    "    \"\"\"\"Sample from a bernouli distribution.\"\"\"\n",
    "\n",
    "    def sample_ber(key, prob):\n",
    "        return jrand.bernoulli(key, p=prob).astype(prob.dtype)\n",
    "    \n",
    "    def gumbel_ber(key, prob, tau):\n",
    "        key_1, key_2 = jrand.split(key)\n",
    "        gumbel_1 = jrand.gumbel(key_1, shape=prob.shape)\n",
    "        gumbel_2 = jrand.gumbel(key_2, shape=prob.shape)\n",
    "        no_logits = (prob * jnp.exp(gumbel_1)) / tau\n",
    "        de_logits = no_logits + ((1. - prob) * jnp.exp(gumbel_2)) / tau\n",
    "        return no_logits / de_logits\n",
    "    \n",
    "    return lax.cond(\n",
    "        training,\n",
    "        lambda _: gumbel_ber(key, prob, tau),\n",
    "        lambda _: sample_ber(key, prob),\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2CModel(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_layers: list[int],\n",
    "        selector_layers: list[int],\n",
    "        feature_indices: list[tuple[int, int]] = None,\n",
    "        pred_fn: Callable = None,\n",
    "        alpha: float = 1e-4, # Sparsity regularization\n",
    "        tau: float = 0.7,\n",
    "        seed: int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator_layers = generator_layers\n",
    "        self.selector_layers = selector_layers\n",
    "        self.feature_indices = feature_indices\n",
    "        self.pred_fn = pred_fn\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        seed = seed or get_config().global_seed\n",
    "        self.seed_generator = SeedGenerator(seed)\n",
    "\n",
    "    def set_features_info(self, feature_indices: list[tuple[int, int]]):\n",
    "        self.feature_indices = feature_indices\n",
    "        # TODO: check if the feature indices are valid\n",
    "\n",
    "    def set_pred_fn(self, pred_fn: Callable):\n",
    "        self.pred_fn = pred_fn\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_feats = len(self.feature_indices)\n",
    "        self.generator = MLP(\n",
    "            sizes=self.generator_layers,\n",
    "            output_size=input_shape[-1],\n",
    "            dropout_rate=0.0,\n",
    "            last_activation=\"linear\",\n",
    "        )\n",
    "        self.selector = MLP(\n",
    "            sizes=n_feats,\n",
    "            output_size=input_shape[-1],\n",
    "            dropout_rate=0.0,\n",
    "            last_activation=\"sigmoid\",\n",
    "        )\n",
    "\n",
    "    def compute_loss(self, inputs, cfs, probs):\n",
    "        y_target = self.pred_fn(inputs).argmin(axis=-1)\n",
    "        y_pred = self.pred_fn(cfs)\n",
    "        validity_loss = keras.losses.sparse_categorical_crossentropy(\n",
    "            y_target, y_pred\n",
    "        )\n",
    "        sparsity = jnp.linalg.norm(probs, p=1) * self.alpha\n",
    "        return validity_loss + sparsity\n",
    "\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        def perturb(cfs, probs, i, start, end):\n",
    "            return (\n",
    "                cfs[:, start:end] * probs[:, i : i + 1] +\n",
    "                inputs[:, start:end] * (1 - probs[:, i : i + 1])\n",
    "            )\n",
    "\n",
    "        select_probs = self.selector(inputs, training=training)\n",
    "        probs = sample_bernouli(\n",
    "            self.seed_generator().next(), select_probs, \n",
    "            tau=self.tau, training=training\n",
    "        )\n",
    "        cfs_logits = self.generator(inputs, training=training)\n",
    "        cfs = sample_categorical(\n",
    "            self.seed_generator().next(), cfs_logits, \n",
    "            tau=self.tau, training=training\n",
    "        )\n",
    "        cfs = jnp.concatenate([\n",
    "                perturb(cfs, probs, i, start, end)\n",
    "                for i, (start, end) in enumerate(self.feature_indices)\n",
    "            ], axis=-1,\n",
    "        )\n",
    "        loss = self.compute_loss(inputs, cfs, probs)\n",
    "        self.add_loss(loss)\n",
    "        return cfs   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def qcut(\n",
    "    x: Array, # Input array\n",
    "    q: int, # Number of quantiles\n",
    "    axis: int = 0, # Axis to quantile\n",
    ") -> tuple[Array, Array]: # (digitized array, quantiles)\n",
    "    \"\"\"Quantile binning.\"\"\"\n",
    "    \n",
    "    # Handle edge cases: empty array or single element\n",
    "    if x.size <= 1:\n",
    "        return jnp.zeros_like(x), jnp.array([])\n",
    "    quantiles = jnp.quantile(x, jnp.linspace(0, 1, q + 1)[1:-1], axis=axis)\n",
    "    # unique_quantiles = jnp.unique(quantiles)\n",
    "    return jnp.digitize(x, quantiles), quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized, quantiles = qcut(jnp.arange(10), 4)\n",
    "assert digitized.shape == (10,)\n",
    "assert quantiles.shape == (3,)\n",
    "assert digitized.min() == 0\n",
    "assert digitized.max() == 3\n",
    "assert jnp.allclose(\n",
    "    quantiles, jnp.array([2.25, 4.5, 6.75])\n",
    ")\n",
    "x_empty = jnp.array([])\n",
    "q = 2\n",
    "digitized_empty, quantiles_empty = qcut(x_empty, q)\n",
    "assert digitized_empty.size == 0 and quantiles_empty.size == 0\n",
    "# Test with single element array\n",
    "x_single = jnp.array([1])\n",
    "digitized_single, quantiles_single = qcut(x_single, q)\n",
    "assert digitized_single.size == 1 and quantiles_single.size == 0\n",
    "\n",
    "# Test with large q value\n",
    "xs = jnp.array([1, 2, 3, 4, 5, 6])\n",
    "q_large = 10\n",
    "_, quantiles_large = qcut(xs, q_large)\n",
    "assert len(quantiles_large) == q_large - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class Discretizer:\n",
    "    q: int = 4 # Number of quantiles\n",
    "\n",
    "    def fit(self, xs: Array):\n",
    "        _, self.quantiles = jax.vmap(qcut, in_axes=(1, None))(xs, self.q)\n",
    "        self.quantiles = jnp.concatenate(\n",
    "            [xs.min(axis=0).reshape(-1, 1), self.quantiles], axis=-1\n",
    "        ) # (n_feats, q)\n",
    "        return self\n",
    "\n",
    "    def transform(self, xs: Array):\n",
    "        if not hasattr(self, \"quantiles\"):\n",
    "            raise ValueError(\"Call fit before transform\")\n",
    "        digitized = jax.vmap(jnp.digitize, in_axes=(1, 0))(xs, self.quantiles[:, 1:]).T\n",
    "        return digitized\n",
    "\n",
    "    def fit_transform(self, xs: Array):\n",
    "        digitized, self.quantiles = jax.vmap(qcut, in_axes=(1, None))(xs, self.q)\n",
    "        self.quantiles = jnp.concatenate(\n",
    "            [xs.min(axis=0).reshape(-1, 1), self.quantiles], axis=-1\n",
    "        )\n",
    "        return digitized.T\n",
    "\n",
    "    def inverse_transform(self, xs: Array):\n",
    "        # TODO\n",
    "        return self.quantiles[xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xs = jnp.arange(40).reshape(5, 8)\n",
    "discretizer = Discretizer(q=4)\n",
    "discretizer.fit(xs)\n",
    "assert discretizer.quantiles.shape == (8, 4)\n",
    "digitized = discretizer.transform(xs)\n",
    "assert digitized.shape == (5, 8)\n",
    "\n",
    "assert jnp.allclose(\n",
    "    digitized,\n",
    "    einops.repeat(jnp.array([0, 1, 2, 3, 3]), 'i -> j i', j=8).T\n",
    ")\n",
    "\n",
    "digitized_ft = discretizer.fit_transform(xs)\n",
    "assert jnp.allclose(\n",
    "    digitized, digitized_ft\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2CConfig(BaseConfig):\n",
    "    generator_layers: list[int]\n",
    "    selector_layers: list[int]\n",
    "    alpha: float = Field(1e-4, description=\"Sparsity regularization.\")\n",
    "    tau: float = Field(0.7, description=\"Temperature for the Gumbel softmax.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def discretize_xs(\n",
    "    xs: Array, # Input array\n",
    "    datamodule: DataModule, # Features list\n",
    "    q: int = 4, # Number of quantiles\n",
    "):\n",
    "    # TODO\n",
    "    discretized_xs = []\n",
    "    cont_quantiles = []\n",
    "    feature_indices = []\n",
    "\n",
    "    for feat, (start, end) in datamodule.features.features_and_indices:\n",
    "        if feat.is_continuous:\n",
    "            discretized, quantiles = qcut(xs[:, start:end], q=q)\n",
    "\n",
    "            \n",
    "        else:\n",
    "            discretized_xs.append(xs[:, start:end])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class L2C(ParametricCFModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Dict | L2CConfig = None,\n",
    "        l2c_model: L2CModel = None,\n",
    "        name: str = \"l2c\",\n",
    "    ):\n",
    "        if config is None:\n",
    "            config = L2CConfig()\n",
    "        config = validate_configs(config, L2CConfig)\n",
    "        name = name or \"l2c\"\n",
    "        self.l2c_model = l2c_model\n",
    "        super().__init__(config=config, name=name)\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, \n",
    "        pred_fn: Callable = None,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if not isinstance(data, DataModule):\n",
    "            raise ValueError(f\"Only support `data` to be `DataModule`, \"\n",
    "                             f\"got type=`{type(data).__name__}` instead.\")\n",
    "        xs_train, ys_train = data['train']\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
