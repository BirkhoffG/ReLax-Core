{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# L2C\n",
    "\n",
    "https://arxiv.org/abs/2209.13446"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp methods.l2c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| include: false\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "from ipynb_path import *\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using JAX backend.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "An NVIDIA GPU may be present on this machine, but a CUDA-enabled jaxlib is not installed. Falling back to cpu.\n"
     ]
    }
   ],
   "source": [
    "#| export\n",
    "from __future__ import annotations\n",
    "from relax.import_essentials import *\n",
    "from relax.methods.base import ParametricCFModule\n",
    "from relax.base import BaseConfig\n",
    "from relax.utils import *\n",
    "from relax.data_utils import Feature, FeaturesList\n",
    "from relax.ml_model import MLP, MLPBlock\n",
    "from relax.data_module import DataModule\n",
    "from keras_core.random import SeedGenerator\n",
    "import einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import torch\n",
    "import relax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def gumbel_softmax(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    logits: Array, # Logits for each class. Shape (batch_size, num_classes)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "):\n",
    "    \"\"\"The Gumbel softmax function.\"\"\"\n",
    "\n",
    "    gumbel_noise = jrand.gumbel(key, shape=logits.shape)\n",
    "    y = logits + gumbel_noise\n",
    "    return jax.nn.softmax(y / tau, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_categorical(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    logits: Array, # Logits for each class. Shape (batch_size, num_classes)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "    training: bool = True, # Apply gumbel softmax if training\n",
    "):\n",
    "    \"\"\"Sample from a categorical distribution.\"\"\"\n",
    "\n",
    "    def sample_cat(key, logits):\n",
    "        cat = jrand.categorical(key, logits=logits, axis=-1)\n",
    "        return jax.nn.one_hot(cat, logits.shape[-1])\n",
    "\n",
    "    return lax.cond(\n",
    "        training,\n",
    "        lambda _: gumbel_softmax(key, logits, tau=tau),\n",
    "        lambda _: sample_cat(key, logits),\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logits = jnp.array([[2.0, 1.0, 0.1], [1.0, 2.0, 3.0]])\n",
    "key = jrand.PRNGKey(0)\n",
    "output = sample_categorical(key, logits, tau=0.5, training=True)\n",
    "assert output.shape == logits.shape\n",
    "assert jnp.allclose(output.sum(axis=-1), 1.0)\n",
    "# low temperature -> one-hot\n",
    "output = sample_categorical(key, logits, tau=0.01, training=True)\n",
    "assert jnp.array_equal(\n",
    "    output.argmax(axis=-1), logits.argmax(axis=-1)\n",
    ")\n",
    "# high temperature -> uniform\n",
    "output = sample_categorical(key, logits, tau=100, training=True)\n",
    "assert jnp.max(output) - jnp.min(output) < 0.5\n",
    "\n",
    "output = sample_categorical(key, logits, tau=0.5, training=False)\n",
    "assert output.shape == logits.shape\n",
    "assert jnp.array_equal(\n",
    "    output.argmax(axis=-1), logits.argmax(axis=-1)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def sample_bernouli(\n",
    "    key: jrand.PRNGKey, # Random key\n",
    "    prob: Array, # Logits for each class. Shape (batch_size, 1)\n",
    "    tau: float, # Temperature for the Gumbel softmax\n",
    "    training: bool = True, # Apply gumbel softmax if training\n",
    ") -> Array:\n",
    "    \"\"\"\"Sample from a bernouli distribution.\"\"\"\n",
    "\n",
    "    def sample_ber(key, prob):\n",
    "        return jrand.bernoulli(key, p=prob).astype(prob.dtype)\n",
    "    \n",
    "    def gumbel_ber(key, prob, tau):\n",
    "        key_1, key_2 = jrand.split(key)\n",
    "        gumbel_1 = jrand.gumbel(key_1, shape=prob.shape)\n",
    "        gumbel_2 = jrand.gumbel(key_2, shape=prob.shape)\n",
    "        no_logits = (prob * jnp.exp(gumbel_1)) / tau\n",
    "        de_logits = no_logits + ((1. - prob) * jnp.exp(gumbel_2)) / tau\n",
    "        return no_logits / de_logits\n",
    "    \n",
    "    return lax.cond(\n",
    "        training,\n",
    "        lambda _: gumbel_ber(key, prob, tau),\n",
    "        lambda _: sample_ber(key, prob),\n",
    "        None,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2CModel(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_layers: list[int],\n",
    "        selector_layers: list[int],\n",
    "        feature_indices: list[tuple[int, int]] = None,\n",
    "        pred_fn: Callable = None,\n",
    "        alpha: float = 1e-4, # Sparsity regularization\n",
    "        tau: float = 0.7,\n",
    "        seed: int = None,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(**kwargs)\n",
    "        self.generator_layers = generator_layers\n",
    "        self.selector_layers = selector_layers\n",
    "        self.feature_indices = feature_indices\n",
    "        self.pred_fn = pred_fn\n",
    "        self.tau = tau\n",
    "        self.alpha = alpha\n",
    "        seed = seed or get_config().global_seed\n",
    "        self.seed_generator = SeedGenerator(seed)\n",
    "\n",
    "    def set_features_info(self, feature_indices: list[tuple[int, int]]):\n",
    "        self.feature_indices = feature_indices\n",
    "        # TODO: check if the feature indices are valid\n",
    "\n",
    "    def set_pred_fn(self, pred_fn: Callable):\n",
    "        self.pred_fn = pred_fn\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        n_feats = len(self.feature_indices)\n",
    "        self.generator = MLP(\n",
    "            sizes=self.generator_layers,\n",
    "            output_size=input_shape[-1],\n",
    "            dropout_rate=0.0,\n",
    "            last_activation=\"linear\",\n",
    "        )\n",
    "        self.selector = MLP(\n",
    "            sizes=self.selector_layers,\n",
    "            output_size=n_feats,\n",
    "            dropout_rate=0.0,\n",
    "            last_activation=\"sigmoid\",\n",
    "        )\n",
    "\n",
    "    def compute_l2c_loss(self, inputs, cfs, probs):\n",
    "        y_target = self.pred_fn(inputs).argmin(axis=-1)\n",
    "        y_pred = self.pred_fn(cfs)\n",
    "        validity_loss = keras.losses.sparse_categorical_crossentropy(\n",
    "            y_target, y_pred\n",
    "        ).mean()\n",
    "        sparsity = jnp.linalg.norm(probs, ord=1) * self.alpha\n",
    "        # self.add_metric(validity_loss, name=\"validity_loss\")\n",
    "        # self.add_metric(sparsity, name=\"sparsity\")\n",
    "        return validity_loss, sparsity\n",
    "    \n",
    "    def perturb(self, inputs, cfs, probs, i, start, end):\n",
    "        return cfs[:, start:end] * probs[:, i : i + 1] + inputs[:, start:end] * (1 - probs[:, i : i + 1])\n",
    "    \n",
    "    def forward(self, inputs, training=False):\n",
    "        select_probs = self.selector(inputs, training=training)\n",
    "        probs = sample_bernouli(\n",
    "            self.seed_generator.next(), select_probs, \n",
    "            tau=self.tau, training=training\n",
    "        )\n",
    "        cfs_logits = self.generator(inputs, training=training)\n",
    "        cfs = sample_categorical(\n",
    "            self.seed_generator.next(), cfs_logits, \n",
    "            tau=self.tau, training=training\n",
    "        )\n",
    "        cfs = jnp.concatenate([\n",
    "                self.perturb(inputs, cfs, probs, i, start, end)\n",
    "                for i, (start, end) in enumerate(self.feature_indices)\n",
    "            ], axis=-1,\n",
    "        )\n",
    "        return cfs, probs\n",
    "    \n",
    "    def call(self, inputs, training=False):\n",
    "        cfs, probs = self.forward(inputs, training=training)\n",
    "        # loss = self.compute_l2c_loss(inputs, cfs, probs)\n",
    "        validity_loss, sparsity = self.compute_l2c_loss(inputs, cfs, probs)\n",
    "        self.add_loss(validity_loss)\n",
    "        self.add_loss(sparsity)\n",
    "        return cfs   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def qcut(\n",
    "    x: Array, # Input array\n",
    "    q: int, # Number of quantiles\n",
    "    axis: int = 0, # Axis to quantile\n",
    ") -> tuple[Array, Array]: # (digitized array, quantiles)\n",
    "    \"\"\"Quantile binning.\"\"\"\n",
    "    \n",
    "    # Handle edge cases: empty array or single element\n",
    "    if x.size <= 1:\n",
    "        return jnp.zeros_like(x), jnp.array([])\n",
    "    quantiles = jnp.quantile(x, jnp.linspace(0, 1, q + 1)[1:-1], axis=axis)\n",
    "    \n",
    "    digitized = jnp.digitize(x, quantiles)\n",
    "    ohe_digitized = jax.nn.one_hot(digitized, q)\n",
    "    quantiles = jnp.concatenate([\n",
    "        x.min(axis=axis, keepdims=True), quantiles, \n",
    "        x.max(axis=axis, keepdims=True)])\n",
    "    quantiles = (quantiles[1:] + quantiles[:-1]) / 2\n",
    "    return ohe_digitized, quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized, quantiles = qcut(jnp.arange(10), 4)\n",
    "assert digitized.shape == (10, 4)\n",
    "assert quantiles.shape == (4,)\n",
    "assert jnp.allclose(\n",
    "    digitized, jax.nn.one_hot(jnp.array([0,0,0,1,1,2,2,3,3,3]), 4)\n",
    ")\n",
    "\n",
    "quantiles_true = jnp.array([0, 2.25, 4.5, 6.75, 9])\n",
    "assert jnp.allclose(\n",
    "    quantiles, (quantiles_true[1:] + quantiles_true[:-1]) / 2\n",
    ")\n",
    "x_empty = jnp.array([])\n",
    "q = 2\n",
    "digitized_empty, quantiles_empty = qcut(x_empty, q)\n",
    "assert digitized_empty.size == 0 and quantiles_empty.size == 0\n",
    "# Test with single element array\n",
    "x_single = jnp.array([1])\n",
    "digitized_single, quantiles_single = qcut(x_single, q)\n",
    "assert digitized_single.size == 1 and quantiles_single.size == 0\n",
    "\n",
    "# Test with large q value\n",
    "xs = jnp.array([1, 2, 3, 4, 5, 6])\n",
    "q_large = 10\n",
    "_, quantiles_large = qcut(xs, q_large)\n",
    "assert len(quantiles_large) == q_large"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def qcut_inverse(\n",
    "    digitized: Array, # Digitized array\n",
    "    quantiles: Array, # Quantiles\n",
    ") -> Array:\n",
    "    \"\"\"Inverse of qcut.\"\"\"\n",
    "    \n",
    "    return digitized @ quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "digitized, quantiles = qcut(jnp.arange(10), 4)\n",
    "quantiles_inv = qcut_inverse(digitized, quantiles)\n",
    "assert quantiles_inv.shape == (10,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def discretize_xs(\n",
    "    xs: Array, # Input array\n",
    "    features_and_indices: list[tuple[Feature, tuple[int, int]]], # Features list\n",
    "    q: int = 4, # Number of quantiles\n",
    ") -> tuple[Array, list[tuple[tuple[int, int], Array]]]: # (discretized array, feature_indices_and_quantiles)\n",
    "    \"\"\"Discretize continuous features.\"\"\"\n",
    "    \n",
    "    discretized_xs = []\n",
    "    feature_indices_and_quantiles = []\n",
    "    discretized_start, discretized_end = 0, 0\n",
    "\n",
    "    for feat, (start, end) in features_and_indices:\n",
    "        if feat.is_categorical:\n",
    "            discretized = xs[:, start:end]\n",
    "            quantiles = None\n",
    "            discretized_end += end - start\n",
    "        else:\n",
    "            discretized, quantiles = qcut(xs[:, start:end].reshape(-1), q=q)\n",
    "            discretized_end += discretized.shape[-1]      \n",
    "        \n",
    "        discretized_xs.append(discretized)\n",
    "        feature_indices_and_quantiles.append(\n",
    "            ((discretized_start, discretized_end), quantiles)\n",
    "        )\n",
    "        discretized_start = discretized_end\n",
    "    discretized_xs = jnp.concatenate(discretized_xs, axis=-1)\n",
    "    return discretized_xs, feature_indices_and_quantiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def inverse_discretize_xs(\n",
    "    xs: Array, # Discretized input array\n",
    "    feature_indices_and_quantiles: list[tuple[tuple[int, int], Array]], # Feature indices and quantiles\n",
    "):\n",
    "    \"\"\"Continutize discretized features.\"\"\"\n",
    "    \n",
    "    continutized_xs = []\n",
    "    for (start, end), quantiles in feature_indices_and_quantiles:\n",
    "        if quantiles is None:\n",
    "            cont_feat = xs[:, start:end]\n",
    "        else:\n",
    "            cont_feat = qcut_inverse(xs[:, start:end], quantiles).reshape(-1, 1)\n",
    "        continutized_xs.append(cont_feat)\n",
    "            \n",
    "    return jnp.concatenate(continutized_xs, axis=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def discretized_pred_fn(\n",
    "    pred_fn: Callable, # Prediction function\n",
    "    feature_indices_and_quantiles: list[tuple[tuple[int, int], Array]], # Feature indices and quantiles\n",
    ") -> Callable[[Array], Array]:\n",
    "    \"\"\"Continutize discretized features.\"\"\"\n",
    "    \n",
    "    def _pred_fn(xs):\n",
    "        continutized_xs = inverse_discretize_xs(xs, feature_indices_and_quantiles)\n",
    "        return pred_fn(continutized_xs)\n",
    "    return _pred_fn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2CConfig(BaseConfig):\n",
    "    generator_layers: list[int] = Field(\n",
    "        [64, 64, 64], description=\"Generator MLP layers.\"\n",
    "    )\n",
    "    selector_layers: list[int] = Field(\n",
    "        [64], description=\"Selector MLP layers.\"\n",
    "    )\n",
    "    lr: float = Field(1e-3, description=\"Model learning rate.\")\n",
    "    opt_name: str = Field(\"adam\", description=\"Optimizer name of training L2C.\")\n",
    "    alpha: float = Field(1e-4, description=\"Sparsity regularization.\")\n",
    "    tau: float = Field(0.7, description=\"Temperature for the Gumbel softmax.\")\n",
    "    q: int = Field(4, description=\"Number of quantiles.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class L2C(ParametricCFModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        config: Dict | L2CConfig = None,\n",
    "        l2c_model: L2CModel = None,\n",
    "        name: str = \"l2c\",\n",
    "    ):\n",
    "        if config is None:\n",
    "            config = L2CConfig()\n",
    "        config = validate_configs(config, L2CConfig)\n",
    "        name = name or \"l2c\"\n",
    "        self.l2c_model = l2c_model\n",
    "        super().__init__(config=config, name=name)\n",
    "\n",
    "    def train(\n",
    "        self, \n",
    "        data: DataModule, \n",
    "        pred_fn: Callable,\n",
    "        batch_size: int = 128,\n",
    "        epochs: int = 10,\n",
    "        **fit_kwargs\n",
    "    ):\n",
    "        if not isinstance(data, DataModule):\n",
    "            raise ValueError(f\"Only support `data` to be `DataModule`, \"\n",
    "                             f\"got type=`{type(data).__name__}` instead.\")\n",
    "        \n",
    "        xs_train, ys_train = data['train']\n",
    "        discretized_xs_train, self.feature_indices_and_quantiles = discretize_xs(\n",
    "            xs_train, data.features.features_and_indices, q=self.config.q\n",
    "        )\n",
    "        pred_fn = discretized_pred_fn(pred_fn, self.feature_indices_and_quantiles)\n",
    "        features_indices = [indices for indices, _ in self.feature_indices_and_quantiles]\n",
    "        \n",
    "        self.l2c_model = L2CModel(\n",
    "            generator_layers=self.config.generator_layers,\n",
    "            selector_layers=self.config.selector_layers,\n",
    "            feature_indices=features_indices,\n",
    "            pred_fn=pred_fn,\n",
    "            alpha=self.config.alpha,\n",
    "            tau=self.config.tau,\n",
    "        )\n",
    "        self.l2c_model.compile(\n",
    "            optimizer=keras.optimizers.get({\n",
    "                    'class_name': self.config.opt_name, \n",
    "                    'config': {'learning_rate': self.config.lr}\n",
    "                }),\n",
    "            loss=None\n",
    "        )\n",
    "        self.l2c_model.fit(\n",
    "            discretized_xs_train, ys_train,\n",
    "            epochs=epochs,\n",
    "            batch_size=batch_size,\n",
    "            **fit_kwargs\n",
    "        )\n",
    "        self._is_trained = True\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "discretize_xs=(750, 40)\n",
      "features_indices=[(0, 4), (4, 8), (8, 12), (12, 16), (16, 20), (20, 24), (24, 28), (28, 32), (32, 36), (36, 40)]\n",
      "continutized_xs.shape:  (128, 10)\n",
      "continutized_xs.shape:  (128, 10)\n",
      "Epoch 1/10\n",
      "continutized_xs.shape:  (128, 10)\n",
      "continutized_xs.shape:  (128, 10)\n",
      "\u001b[1m1/6\u001b[0m \u001b[32m━━━\u001b[0m\u001b[37m━━━━━━━━━━━━━━━━━\u001b[0m \u001b[1m7s\u001b[0m 2s/step - loss: 1.9377continutized_xs.shape:  (110, 10)\n",
      "continutized_xs.shape:  (110, 10)\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 364ms/step - loss: 1.9441\n",
      "Epoch 2/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8550 \n",
      "Epoch 3/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.8019\n",
      "Epoch 4/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.4649\n",
      "Epoch 5/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.2174\n",
      "Epoch 6/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 1.1445\n",
      "Epoch 7/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.9285\n",
      "Epoch 8/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.7578\n",
      "Epoch 9/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.6021\n",
      "Epoch 10/10\n",
      "\u001b[1m6/6\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - loss: 0.4863\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<__main__.L2C>"
      ]
     },
     "execution_count": null,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dm = relax.load_data('dummy')\n",
    "ml_module = relax.load_ml_module('dummy')\n",
    "l2c = L2C()\n",
    "l2c.train(dm, ml_module.pred_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @dataclass\n",
    "# class Discretizer:\n",
    "#     q: int = 4 # Number of quantiles\n",
    "\n",
    "#     def fit(self, xs: Array):\n",
    "#         _, self.quantiles = jax.vmap(qcut, in_axes=(1, None))(xs, self.q)\n",
    "#         self.quantiles = jnp.concatenate(\n",
    "#             [xs.min(axis=0).reshape(-1, 1), self.quantiles], axis=-1\n",
    "#         ) # (n_feats, q)\n",
    "#         return self\n",
    "\n",
    "#     def transform(self, xs: Array):\n",
    "#         if not hasattr(self, \"quantiles\"):\n",
    "#             raise ValueError(\"Call fit before transform\")\n",
    "#         digitized = jax.vmap(jnp.digitize, in_axes=(1, 0))(xs, self.quantiles[:, 1:]).T\n",
    "#         return digitized\n",
    "\n",
    "#     def fit_transform(self, xs: Array):\n",
    "#         digitized, self.quantiles = jax.vmap(qcut, in_axes=(1, None))(xs, self.q)\n",
    "#         self.quantiles = jnp.concatenate(\n",
    "#             [xs.min(axis=0).reshape(-1, 1), self.quantiles], axis=-1\n",
    "#         )\n",
    "#         return digitized.T\n",
    "\n",
    "#     def inverse_transform(self, xs: Array):\n",
    "#         return self.quantiles[xs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xs = jnp.arange(40).reshape(5, 8)\n",
    "# discretizer = Discretizer(q=4)\n",
    "# discretizer.fit(xs)\n",
    "# assert discretizer.quantiles.shape == (8, 4)\n",
    "# digitized = discretizer.transform(xs)\n",
    "# assert digitized.shape == (5, 8)\n",
    "\n",
    "# assert jnp.allclose(\n",
    "#     digitized,\n",
    "#     einops.repeat(jnp.array([0, 1, 2, 3, 3]), 'i -> j i', j=8).T\n",
    "# )\n",
    "\n",
    "# digitized_ft = discretizer.fit_transform(xs)\n",
    "# assert jnp.allclose(\n",
    "#     digitized, digitized_ft\n",
    "# )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
