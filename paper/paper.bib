@inproceedings{ustun2019actionable,
    author = {Ustun, Berk and Spangher, Alexander and Liu, Yang},
    title = {Actionable Recourse in Linear Classification},
    year = {2019},
    isbn = {9781450361255},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3287560.3287566},
    doi = {10.1145/3287560.3287566},
    booktitle = {Proceedings of the Conference on Fairness, Accountability, and Transparency},
    pages = {10–19},
    numpages = {10},
    keywords = {accountability, audit, classification, credit scoring, integer programming, recourse},
    location = {Atlanta, GA, USA},
    series = {FAT* '19}
}
@article{wachter2017counterfactual,
  title={Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR},
  author={Wachter, Sandra and Mittelstadt, Brent and Russell, Chris},
  journal={Cybersecurity},
  year={2017},
  doi={10.2139/ssrn.3063289},
  url={https://api.semanticscholar.org/CorpusID:3995299}
}
@article{verma2020counterfactual,
  title={Counterfactual Explanations for Machine Learning: A Review},
  author={Verma, Sahil and Dickerson, John and Hines, Keegan},
  journal={arXiv preprint arXiv:2010.10596},
  year={2020}
}
@article{stepin2021survey,
  author={Stepin, Ilia and Alonso, Jose M. and Catala, Alejandro and Pereira-Fariña, Martín},
  journal={IEEE Access}, 
  title={A Survey of Contrastive and Counterfactual Explanation Generation Methods for Explainable Artificial Intelligence}, 
  year={2021},
  volume={9},
  number={},
  pages={11974-12001},
  keywords={Cognition;Artificial intelligence;Training;Terminology;Taxonomy;Systematics;Signal to noise ratio;Computational intelligence;contrastive explanations;counterfactuals;explainable artificial intelligence;systematic literature review},
  doi={10.1109/ACCESS.2021.3051315}
}
@article{karimi2020survey,
  author = {Karimi, Amir-Hossein and Barthe, Gilles and Sch\"{o}lkopf, Bernhard and Valera, Isabel},
    title = {A Survey of Algorithmic Recourse: Contrastive Explanations and Consequential Recommendations},
    year = {2022},
    issue_date = {May 2023},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    volume = {55},
    number = {5},
    issn = {0360-0300},
    url = {https://doi.org/10.1145/3527848},
    doi = {10.1145/3527848},
    abstract = {Machine learning is increasingly used to inform decision making in sensitive situations where decisions have consequential effects on individuals’ lives. In these settings, in addition to requiring models to be accurate and robust, socially relevant values such as fairness, privacy, accountability, and explainability play an important role in the adoption and impact of said technologies. In this work, we focus on algorithmic recourse, which is concerned with providing explanations and recommendations to individuals who are unfavorably treated by automated decision-making systems. We first perform an extensive literature review, and align the efforts of many authors by presenting unified definitions, formulations, and solutions to recourse. Then, we provide an overview of the prospective research directions toward which the community may engage, challenging existing assumptions and making explicit connections to other ethical challenges such as security, privacy, and fairness.},
    journal = {ACM Comput. Surv.},
    month = dec,
    articleno = {95},
    numpages = {29},
    keywords = {contrastive explanations and consequential recommendations, Algorithmic recourse}
}
@inproceedings{binns2018s,
    author = {Binns, Reuben and Van Kleek, Max and Veale, Michael and Lyngs, Ulrik and Zhao, Jun and Shadbolt, Nigel},
    title = { 'It's Reducing a Human Being to a Percentage': Perceptions of Justice in Algorithmic Decisions},
    year = {2018},
    isbn = {9781450356206},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3173574.3173951},
    doi = {10.1145/3173574.3173951},
    abstract = {Data-driven decision-making consequential to individuals raises important questions of accountability and justice. Indeed, European law provides individuals limited rights to 'meaningful information about the logic' behind significant, autonomous decisions such as loan approvals, insurance quotes, and CV filtering. We undertake three experimental studies examining people's perceptions of justice in algorithmic decision-making under different scenarios and explanation styles. Dimensions of justice previously observed in response to human decision-making appear similarly engaged in response to algorithmic decisions. Qualitative analysis identified several concerns and heuristics involved in justice perceptions including arbitrariness, generalisation, and (in)dignity. Quantitative analysis indicates that explanation styles primarily matter to justice perceptions only when subjects are exposed to multiple different styles---under repeated exposure of one style, scenario effects obscure any explanation effects. Our results suggests there may be no 'best' approach to explaining algorithmic decisions, and that reflection on their automated nature both implicates and mitigates justice dimensions.},
    booktitle = {Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems},
    pages = {1–14},
    numpages = {14},
    keywords = {transparency, machine learning, justice, fairness, explanation, algorithmic decision-making},
    location = {Montreal QC, Canada},
    series = {CHI '18}
}
@article{miller2019explanation,
  author       = {Tim Miller},
  title        = {Explanation in artificial intelligence: Insights from the social sciences},
  journal      = {Artif. Intell.},
  volume       = {267},
  pages        = {1--38},
  year         = {2019},
  url          = {https://doi.org/10.1016/j.artint.2018.07.007},
  doi          = {10.1016/J.ARTINT.2018.07.007},
  timestamp    = {Thu, 25 May 2023 12:52:41 +0200},
  biburl       = {https://dblp.org/rec/journals/ai/Miller19.bib},
  bibsource    = {dblp computer science bibliography, https://dblp.org}
}
@inproceedings{Bhatt20explainable,
author = {Bhatt, Umang and Xiang, Alice and Sharma, Shubham and Weller, Adrian and Taly, Ankur and Jia, Yunhan and Ghosh, Joydeep and Puri, Ruchir and Moura, Jos\'{e} M. F. and Eckersley, Peter},
title = {Explainable Machine Learning in Deployment},
year = {2020},
isbn = {9781450369367},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3351095.3375624},
doi = {10.1145/3351095.3375624},
abstract = {Explainable machine learning offers the potential to provide stakeholders with insights into model behavior by using various methods such as feature importance scores, counterfactual explanations, or influential training data. Yet there is little understanding of how organizations use these methods in practice. This study explores how organizations view and use explainability for stakeholder consumption. We find that, currently, the majority of deployments are not for end users affected by the model but rather for machine learning engineers, who use explainability to debug the model itself. There is thus a gap between explainability in practice and the goal of transparency, since explanations primarily serve internal stakeholders rather than external ones. Our study synthesizes the limitations of current explainability techniques that hamper their use for end users. To facilitate end user interaction, we develop a framework for establishing clear goals for explainability. We end by discussing concerns raised regarding explainability.},
booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
pages = {648–657},
numpages = {10},
keywords = {explainability, machine learning, deployed systems, transparency, qualitative study},
location = {Barcelona, Spain},
series = {FAT* '20}
}
@inproceedings{mothilal2020explaining,
    author = {Mothilal, Ramaravind K. and Sharma, Amit and Tan, Chenhao},
    title = {Explaining machine learning classifiers through diverse counterfactual explanations},
    year = {2020},
    isbn = {9781450369367},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3351095.3372850},
    doi = {10.1145/3351095.3372850},
    abstract = {Post-hoc explanations of machine learning models are crucial for people to understand and act on algorithmic predictions. An intriguing class of explanations is through counterfactuals, hypothetical examples that show people how to obtain a different prediction. We posit that effective counterfactual explanations should satisfy two properties: feasibility of the counterfactual actions given user context and constraints, and diversity among the counterfactuals presented. To this end, we propose a framework for generating and evaluating a diverse set of counterfactual explanations based on determinantal point processes. To evaluate the actionability of counterfactuals, we provide metrics that enable comparison of counterfactual-based methods to other local explanation methods. We further address necessary tradeoffs and point to causal implications in optimizing for counterfactuals. Our experiments on four real-world datasets show that our framework can generate a set of counterfactuals that are diverse and well approximate local decision boundaries, outperforming prior approaches to generating diverse counterfactuals. We provide an implementation of the framework at https://github.com/microsoft/DiCE.},
    booktitle = {Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency},
    pages = {607–617},
    numpages = {11},
    location = {Barcelona, Spain},
    series = {FAT* '20}
}
@article{upadhyay2021towards,
    author = {Upadhyay, Sohini and Joshi, Shalmali and Lakkaraju, Himabindu},
    title = {Towards robust and reliable algorithmic recourse},
    year = {2024},
    isbn = {9781713845393},
    publisher = {Curran Associates Inc.},
    address = {Red Hook, NY, USA},
    abstract = {As predictive models are increasingly being deployed in high-stakes decision making (e.g., loan approvals), there has been growing interest in post-hoc techniques which provide recourse to affected individuals. These techniques generate recourses under the assumption that the underlying predictive model does not change. However, in practice, models are often regularly updated for a variety of reasons (e.g., dataset shifts), thereby rendering previously prescribed recourses ineffective. To address this problem, we propose a novel framework, RObust Algorithmic Recourse (ROAR), that leverages adversarial training for finding recourses that are robust to model shifts. To the best of our knowledge, this work proposes the first ever solution to this critical problem. We also carry out theoretical analysis which underscores the importance of constructing recourses that are robust to model shifts: 1) We quantify the probability of invalidation for recourses generated without accounting for model shifts. 2) We prove that the additional cost incurred due to the robust recourses output by our framework is bounded. Experimental evaluation on multiple synthetic and real-world datasets demonstrates the efficacy of the proposed framework.},
    booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
    articleno = {1294},
    numpages = {12},
    series = {NIPS '21}
}
@inproceedings{vo2023feature,
    author = {Vo, Vy and Le, Trung and Nguyen, Van and Zhao, He and Bonilla, Edwin V. and Haffari, Gholamreza and Phung, Dinh},
    title = {Feature-based Learning for Diverse and Privacy-Preserving Counterfactual Explanations},
    year = {2023},
    isbn = {9798400701030},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3580305.3599343},
    doi = {10.1145/3580305.3599343},
    booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
    pages = {2211–2222},
    numpages = {12},
    keywords = {algorithmic recourse, explainable ai, privacy},
    location = {Long Beach, CA, USA},
    series = {KDD '23}
}
@inproceedings{guo2021counternet,
    author = {Guo, Hangzhi and Nguyen, Thanh H. and Yadav, Amulya},
    title = {CounterNet: End-to-End Training of Prediction Aware Counterfactual Explanations},
    year = {2023},
    isbn = {9798400701030},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3580305.3599290},
    doi = {10.1145/3580305.3599290},
    abstract = {This work presents CounterNet, a novel end-to-end learning framework which integrates Machine Learning (ML) model training and the generation of corresponding counterfactual (CF) explanations into a single end-to-end pipeline. Counterfactual explanations offer a contrastive case, i.e., they attempt to find the smallest modification to the feature values of an instance that changes the prediction of the ML model on that instance to a predefined output. Prior techniques for generating CF explanations suffer from two major limitations: (i) all of them are post-hoc methods designed for use with proprietary ML models --- as a result, their procedure for generating CF explanations is uninformed by the training of the ML model, which leads to misalignment between model predictions and explanations; and (ii) most of them rely on solving separate time-intensive optimization problems to find CF explanations for each input data point (which negatively impacts their runtime). This work makes a novel departure from the prevalent post-hoc paradigm (of generating CF explanations) by presenting CounterNet, an end-to-end learning framework which integrates predictive model training and the generation of counterfactual (CF) explanations into a single pipeline. Unlike post-hoc methods, CounterNet enables the optimization of the CF explanation generation only once together with the predictive model. We adopt a block-wise coordinate descent procedure which helps in effectively training CounterNet's network. Our extensive experiments on multiple real-world datasets show that CounterNet generates high-quality predictions, and consistently achieves 100\% CF validity and low proximity scores (thereby achieving a well-balanced cost-invalidity trade-off) for any new input instance, and runs 3X faster than existing state-of-the-art baselines.},
    booktitle = {Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining},
    pages = {577–589},
    numpages = {13},
    keywords = {algorithmic recourse, counterfactual explanation, explainable artificial intelligence, interpretability},
    location = {Long Beach, CA, USA},
    series = {KDD '23}
}

@inproceedings{guo2023rocoursenet,
author = {Guo, Hangzhi and Jia, Feiran and Chen, Jinghui and Squicciarini, Anna and Yadav, Amulya},
title = {RoCourseNet: Robust Training of a Prediction Aware Recourse Model},
year = {2023},
isbn = {9798400701245},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3583780.3615040},
doi = {10.1145/3583780.3615040},
abstract = {Counterfactual (CF) explanations for machine learning (ML) models are preferred by end-users, as they explain the predictions of ML models by providing a recourse (or contrastive) case to individuals who are adversely impacted by predicted outcomes. Existing CF explanation methods generate recourses under the assumption that the underlying target ML model remains stationary over time. However, due to commonly occurring distributional shifts in training data, ML models constantly get updated in practice, which might render previously generated recourses invalid and diminish end-users trust in our algorithmic framework. To address this problem, we propose RoCourseNet, a training framework that jointly optimizes predictions and recourses that are robust to future data shifts. This work contains four key contributions: (1) We formulate the robust recourse generation problem as a tri-level optimization problem which consists of two sub-problems: (i) a bi-level problem that finds the worst-case adversarial shift in the training data, and (ii) an outer minimization problem to generate robust recourses against this worst-case shift. (2) We leverage adversarial training to solve this tri-level optimization problem by: (i) proposing a novel virtual data shift (VDS) algorithm to find worst-case shifted ML models via explicitly considering the worst-case data shift in the training dataset, and (ii) a block-wise coordinate descent procedure to optimize for prediction and corresponding robust recourses. (3) We evaluate RoCourseNet's performance on three real-world datasets, and show that RoCourseNet consistently achieves more than 96\% robust validity and outperforms state-of-the-art baselines by at least 10\% in generating robust CF explanations. (4) Finally, we generalize the RoCourseNet framework to accommodate any parametric post-hoc methods for improving robust validity.},
booktitle = {Proceedings of the 32nd ACM International Conference on Information and Knowledge Management},
pages = {619–628},
numpages = {10},
keywords = {interpretability, explainable artificial intelligence, counterfactual explanation, algorithmic recourse, adversarial machine learning},
location = {Birmingham, United Kingdom},
series = {CIKM '23}
}
@article{pawelczyk2021carla,
  title={CARLA: A {Python} Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms},
  author={Martin Pawelczyk and Sascha Bielawski and Jan van den Heuvel and Tobias Richter and Gjergji Kasneci},
  journal={ArXiv},
  year={2021},
  volume={abs/2108.00783},
  url={https://api.semanticscholar.org/CorpusID:236772193}
}
@article{klaise2021alibi,
  author  = {Janis Klaise and Arnaud Van Looveren and Giovanni Vacanti and Alexandru Coca},
  title   = {Alibi Explain: Algorithms for Explaining Machine Learning Models},
  journal = {Journal of Machine Learning Research},
  year    = {2021},
  volume  = {22},
  number  = {181},
  pages   = {1-7},
  url     = {http://jmlr.org/papers/v22/21-0017.html}
}
@misc{kohavi1996uci,
  title={UCI Machine Learning Repository: Adult Data Set},
  author={Kohavi, R and Becker, B},
  year={1996},
  journal={1996-05-01)[2014-10-01]. http: ff archive, ies. uci. edu/ml/data-sets/Adult}
}
@software{jax2018github,
  author = {James Bradbury and Roy Frostig and Peter Hawkins and Matthew James Johnson and Chris Leary and Dougal Maclaurin and George Necula and Adam Paszke and Jake Vander{P}las and Skye Wanderman-{M}ilne and Qiao Zhang},
  title = {{JAX}: composable transformations of {P}ython+{N}um{P}y programs},
  url = {http://github.com/google/jax},
  version = {0.4.10},
  year = {2018},
}
@inproceedings{frostig2018jax,
    title	= {Compiling machine learning programs via high-level tracing},
    author	= {Roy Frostig and Matthew Johnson and Chris Leary},
    year	= {2018},
    URL	= {https://mlsys.org/Conferences/doc/2018/146.pdf}
}
@article{laugel2017inverse,
  title={Inverse classification for comparison-based interpretability in machine learning},
  author={Laugel, Thibault and Lesot, Marie-Jeanne and Marsala, Christophe and Renard, Xavier and Detyniecki, Marcin},
  journal={arXiv preprint arXiv:1712.08443},
  year={2017}
}
@article{van2019interpretable,
    author = {Van Looveren, Arnaud and Klaise, Janis},
    title = {Interpretable Counterfactual Explanations Guided by Prototypes},
    year = {2021},
    isbn = {978-3-030-86519-1},
    publisher = {Springer-Verlag},
    address = {Berlin, Heidelberg},
    url = {https://doi.org/10.1007/978-3-030-86520-7_40},
    doi = {10.1007/978-3-030-86520-7_40},
    abstract = {We propose a fast, model agnostic method for finding interpretable counterfactual explanations of classifier predictions by using class prototypes. We show that class prototypes, obtained using either an encoder or through class specific k-d trees, significantly speed up the search for counterfactual instances and result in more interpretable explanations. We quantitatively evaluate interpretability of the generated counterfactuals to illustrate the effectiveness of our method on an image and tabular dataset, respectively MNIST and Breast Cancer Wisconsin (Diagnostic). Additionally, we propose a principled approach to handle categorical variables and illustrate our method on the Adult (Census) dataset. Our method also eliminates the computational bottleneck that arises because of numerical gradient evaluation for black box models.},
    booktitle = {Machine Learning and Knowledge Discovery in Databases. Research Track: European Conference, ECML PKDD 2021, Bilbao, Spain, September 13–17, 2021, Proceedings, Part II},
    pages = {650–665},
    numpages = {16},
    keywords = {Interpretation, Transparency/Explainability, Counterfactual explanations},
    location = {Bilbao, Spain}
}
@inproceedings{pawelczyk2020learning,
    author = {Pawelczyk, Martin and Broelemann, Klaus and Kasneci, Gjergji},
    title = {Learning Model-Agnostic Counterfactual Explanations for Tabular Data},
    year = {2020},
    isbn = {9781450370233},
    publisher = {Association for Computing Machinery},
    address = {New York, NY, USA},
    url = {https://doi.org/10.1145/3366423.3380087},
    doi = {10.1145/3366423.3380087},
    abstract = {Counterfactual explanations can be obtained by identifying the smallest change made to an input vector to influence a prediction in a positive way from a user’s viewpoint; for example, from ’loan rejected’ to ’awarded’ or from ’high risk of cardiovascular disease’ to ’low risk’. Previous approaches would not ensure that the produced counterfactuals be proximate (i.e., not local outliers) and connected to regions with substantial data density (i.e., close to correctly classified observations), two requirements known as counterfactual faithfulness. Our contribution is twofold. First, drawing ideas from the manifold learning literature, we develop a framework, called C-CHVAE, that generates faithful counterfactuals. Second, we suggest to complement the catalog of counterfactual quality measures using a criterion to quantify the degree of difficulty for a certain counterfactual suggestion. Our real world experiments suggest that faithful counterfactuals come at the cost of higher degrees of difficulty.},
    booktitle = {Proceedings of The Web Conference 2020},
    pages = {3126–3132},
    numpages = {7},
    keywords = {Counterfactual explanations, Interpretability, Transparency},
    location = {Taipei, Taiwan},
    series = {WWW '20}
}
@inproceedings{antoran2021clue,
title={Getting a {CLUE}: A  Method for Explaining Uncertainty Estimates},
author={Javier Antoran and Umang Bhatt and Tameem Adel and Adrian Weller and Jos{\'e} Miguel Hern{\'a}ndez-Lobato},
booktitle={International Conference on Learning Representations},
year={2021},
url={https://openreview.net/forum?id=XSLF1XFq5h}
}
@article{mahajan2019preserving,
  title={Preserving causal constraints in counterfactual explanations for machine learning classifiers},
  author={Mahajan, Divyat and Tan, Chenhao and Sharma, Amit},
  journal={arXiv preprint arXiv:1912.03277},
  year={2019}
}
@article{ding2021retiring,
author = {Ding, Frances and Hardt, Moritz and Miller, John and Schmidt, Ludwig},
title = {Retiring adult: new datasets for fair machine learning},
year = {2024},
isbn = {9781713845393},
publisher = {Curran Associates Inc.},
address = {Red Hook, NY, USA},
abstract = {Although the fairness community has recognized the importance of data, researchers in the area primarily rely on UCI Adult when it comes to tabular data. Derived from a 1994 US Census survey, this dataset has appeared in hundreds of research papers where it served as the basis for the development and comparison of many algorithmic fairness interventions. We reconstruct a superset of the UCI Adult data from available US Census sources and reveal idiosyncrasies of the UCI Adult dataset that limit its external validity. Our primary contribution is a suite of new datasets derived from US Census surveys that extend the existing data ecosystem for research on fair machine learning. We create prediction tasks relating to income, employment, health, transportation, and housing. The data span multiple years and all states of the United States, allowing researchers to study temporal shift and geographic variation. We highlight a broad initial sweep of new empirical insights relating to trade-offs between fairness criteria, performance of algorithmic interventions, and the role of distribution shift based on our new datasets. Our findings inform ongoing debates, challenge some existing narratives, and point to future research directions.},
booktitle = {Proceedings of the 35th International Conference on Neural Information Processing Systems},
articleno = {496},
numpages = {13},
series = {NIPS '21}
}
